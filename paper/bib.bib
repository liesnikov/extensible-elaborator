@article{abelExplicitSubstitutionsContextual2010a,
  title = {Explicit {{Substitutions}} for {{Contextual Type Theory}}},
  author = {Abel, Andreas and Pientka, Brigitte},
  date = {2010-09-11},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  shortjournal = {Electron. Proc. Theor. Comput. Sci.},
  volume = {34},
  eprint = {1009.2789},
  eprinttype = {arxiv},
  pages = {5--20},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.34.3},
  url = {http://arxiv.org/abs/1009.2789},
  urldate = {2023-04-10},
  abstract = {In this paper, we present an explicit substitution calculus which distinguishes between ordinary bound variables and meta-variables. Its typing discipline is derived from contextual modal type theory. We first present a dependently typed lambda calculus with explicit substitutions for ordinary variables and explicit meta-substitutions for meta-variables. We then present a weak head normalization procedure which performs both substitutions lazily and in a single pass thereby combining substitution walks for the two different classes of variables. Finally, we describe a bidirectional type checking algorithm which uses weak head normalization and prove soundness.},
  keywords = {archived,Computer Science - Logic in Computer Science,Computer Science - Programming Languages,F.4.1,I.2.3},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Abel_Pientka/2010/Abel_Pientka_2010_Explicit Substitutions for Contextual Type Theory2.pdf}
}

@inproceedings{abelExtensionMartinLofType2016,
  title = {An {{Extension}} of {{Martin-Löf Type Theory}} with {{Sized Types}}},
  author = {Abel, Andreas and Winterhalter, Théo},
  date = {2016-05-23/2016-05-26},
  pages = {2},
  location = {{Novi Sad, Serbia}},
  abstract = {We present a dependent type theory for which termination checking is entirely type-based, through the use of sized types. Sizes are absent from terms to ensure they are irrelevant for computation and reasoning. The novelty of our approach is the combination of first class size quantification ∀ i → T and dependent types, justified by a predicative semantics.},
  eventtitle = {{{TYPES}}'16},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Abel_Winterhalter/2016/Abel_Winterhalter_2016_An Extension of Martin-Löf Type Theory with Sized Types.pdf}
}

@inproceedings{abelHigherOrderDynamicPattern2011,
  title = {Higher-{{Order Dynamic Pattern Unification}} for {{Dependent Types}} and {{Records}}},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Abel, Andreas and Pientka, Brigitte},
  editor = {Ong, Luke},
  date = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {10--26},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-21691-6_5},
  abstract = {While higher-order pattern unification for the λΠ-calculus is decidable and unique unifiers exists, we face several challenges in practice: 1) the pattern fragment itself is too restrictive for many applications; this is typically addressed by solving sub-problems which satisfy the pattern restriction eagerly but delay solving sub-problems which are non-patterns until we have accumulated more information. This leads to a dynamic pattern unification algorithm. 2) Many systems implement λΠΣ calculus and hence the known pattern unification algorithms for λΠ are too restrictive.},
  isbn = {978-3-642-21691-6},
  langid = {english},
  keywords = {Deductive System,Free Variable,Logical Framework,Pattern Fragment,Typing Rule},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Abel_Pientka/2011/Abel_Pientka_2011_Higher-Order Dynamic Pattern Unification for Dependent Types and Records.pdf}
}

@article{abelUnifiedViewModalities2020,
  title = {A Unified View of Modalities in Type Systems},
  author = {Abel, Andreas and Bernardy, Jean-Philippe},
  date = {2020-08-02},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {4},
  pages = {90:1--90:28},
  doi = {10.1145/3408972},
  url = {https://doi.org/10.1145/3408972},
  urldate = {2022-07-04},
  abstract = {We propose to unify the treatment of a broad range of modalities in typed lambda calculi. We do so by defining a generic structure of modalities, and show that this structure arises naturally from the structure of intuitionistic logic, and as such finds instances in a wide range of type systems previously described in literature. Despite this generality, this structure has a rich metatheory, which we expose.},
  issue = {ICFP},
  keywords = {linear types,modal logic,subtyping},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Abel_Bernardy/2020/Abel_Bernardy_2020_A unified view of modalities in type systems.pdf}
}

@software{alekseykligerUnboundgenerics2022,
  title = {Unbound-Generics},
  author = {{Aleksey Kliger}},
  date = {2022-06-24},
  url = {http://github.com/lambdageek/unbound-generics},
  urldate = {2022-09-20},
  abstract = {Support for programming with names and binders using GHC Generics}
}

@inproceedings{allaisTypOSOperatingSystem2022a,
  ids = {allaisTypOSOperatingSystem2022a; https://web.archive.org/web/20220627195901/https://types22.inria.fr/files/2022/06/TYPES_2022_paper_31.pdf},
  title = {{{TypOS}}: {{An Operating System}} for {{Typechecking Actors}}},
  author = {Allais, Guillaume and Altenmuller, Malin and McBride, Conor and Nakov, Georgi and Forsberg, Nordvall and Roy, Craig},
  date = {2022-06-22},
  pages = {3},
  location = {{Nantes, France}},
  url = {https://types22.inria.fr/files/2022/06/TYPES_2022_paper_31.pdf},
  abstract = {Introduction We report work in progress on TypOS, a domain-specific language for experi- menting with typecheckers and elaborators. Our remit is similar to those of other domain-specific languages such as Andromeda [1], PLT Redex [ 3], or Turnstyle+ [ 2]. However, we try to minimise demands on the order in which subproblems are encountered and constraints are solved: when programs contain holes, and constraints are complex, it helps to be flexible about where to make progress. TypOS tackles typing tasks by spawning a network of concurrent actors [4], each responsible for one node of the syntax tree, communicating with its parent and child nodes via channels. Constraints demanded by one actor may generate enough information (by solving metavariables) for other actors to unblock. Metavariables thus provide a secondary and asynchronous means of communication, creating subtle difficulties modelling actor resumptions as host-language functions, because metavariables unsolved at the time of a resumption’s creation may be solved by the time it is invoked. Thus forced into a more syntactic representation of suspended processes, we decided to explore what opportunities a new language could bring.},
  eventtitle = {{{TYPES}}},
  langid = {english},
  keywords = {archived},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Allais et al/2022/Allais et al_2022_TypOS.pdf}
}

@inproceedings{aspertiCraftingProofAssistant2007,
  title = {Crafting a {{Proof Assistant}}},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {Asperti, Andrea and Coen, Claudio Sacerdoti and Tassi, Enrico and Zacchiroli, Stefano},
  editor = {Altenkirch, Thorsten and McBride, Conor},
  date = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {18--32},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74464-1_2},
  abstract = {Proof assistants are complex applications whose development has never been properly systematized or documented. This work is a contribution in this direction, based on our experience with the development of Matita: a new interactive theorem prover based—as Coq—on the Calculus of Inductive Constructions (CIC). In particular, we analyze its architecture focusing on the dependencies of its components, how they implement the main functionalities, and their degree of reusability.},
  isbn = {978-3-540-74464-1},
  langid = {english},
  keywords = {Architectural Solution,Level Term,Logical Framework,Proof Assistant,Theorem Prover},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Asperti et al/2007/Asperti et al_2007_Crafting a Proof Assistant.pdf}
}

@inproceedings{aspertiMatitaInteractiveTheorem2011,
  title = {The {{Matita Interactive Theorem Prover}}},
  booktitle = {Automated {{Deduction}} – {{CADE-23}}},
  author = {Asperti, Andrea and Ricciotti, Wilmer and Sacerdoti Coen, Claudio and Tassi, Enrico},
  editor = {Bjørner, Nikolaj and Sofronie-Stokkermans, Viorica},
  date = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {64--69},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-22438-6_7},
  abstract = {Matita is an interactive theorem prover being developed by the Helm team at the University of Bologna. Its stable version 0.5.x may be downloaded at http://matita.cs.unibo.it. The tool originated in the European project MoWGLI as a set of XML-based tools aimed to provide a mathematician-friendly web-interface to repositories of formal mathematical knoweldge, supporting advanced content-based functionalities for querying, searching and browsing the library. It has since then evolved into a fully fledged ITP, specifically designed as a light-weight, but competitive system, particularly suited for the assessment of innovative ideas, both at foundational and logical level. In this paper, we give an account of the whole system, its peculiarities and its main applications.},
  isbn = {978-3-642-22438-6},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Asperti et al/2011/Asperti et al_2011_The Matita Interactive Theorem Prover.pdf}
}

@article{aspertiUserInteractionMatita2007,
  title = {User {{Interaction}} with the {{Matita Proof Assistant}}},
  author = {Asperti, Andrea and Sacerdoti Coen, Claudio and Tassi, Enrico and Zacchiroli, Stefano},
  date = {2007-08-01},
  journaltitle = {Journal of Automated Reasoning},
  shortjournal = {J Autom Reasoning},
  volume = {39},
  number = {2},
  pages = {109--139},
  issn = {1573-0670},
  doi = {10.1007/s10817-007-9070-5},
  url = {https://doi.org/10.1007/s10817-007-9070-5},
  urldate = {2022-05-23},
  abstract = {Matita is a new, document-centric, tactic-based interactive theorem prover. This paper focuses on some of the distinctive features of the user interaction with Matita, characterized mostly by the organization of the library as a searchable knowledge base, the emphasis on a high-quality notational rendering, and the complex interplay between syntax, presentation, and semantics.},
  langid = {english},
  keywords = {Interactive theorem proving,Mathematical knowledge management,matita,Proof assistant},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Asperti et al/2007/Asperti et al_2007_User Interaction with the Matita Proof Assistant.pdf}
}

@unpublished{bauerDesignImplementationAndromeda2018,
  title = {Design and {{Implementation}} of the {{Andromeda Proof Assistant}}},
  author = {Bauer, Andrej and Gilbert, Gaëtan and Haselwarter, Philipp G. and Pretnar, Matija and Stone, Christopher A.},
  editora = {Wagner, Michael},
  editoratype = {collaborator},
  date = {2018},
  eprint = {1802.06217},
  eprinttype = {arxiv},
  pages = {31 pages},
  publisher = {{Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany}},
  doi = {10.4230/LIPICS.TYPES.2016.5},
  url = {http://drops.dagstuhl.de/opus/volltexte/2018/9857/},
  urldate = {2022-07-22},
  abstract = {Andromeda is an LCF-style proof assistant where the user builds derivable judgments by writing code in a meta-level programming language AML. The only trusted component of Andromeda is a minimalist nucleus (an implementation of the inference rules of an object-level type theory), which controls construction and decomposition of type-theoretic judgments. Since the nucleus does not perform complex tasks like equality checking beyond syntactic equality, this responsibility is delegated to the user, who implements one or more equality checking procedures in the meta-language. The AML interpreter requests witnesses of equality from user code using the mechanism of algebraic operations and handlers. Dynamic checks in the nucleus guarantee that no invalid object-level derivations can be constructed.},
  langid = {english},
  keywords = {03B15,andromeda,Computer Science - Logic in Computer Science},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Bauer et al/2018/Bauer et al_2018_Design and Implementation of the Andromeda Proof Assistant2.pdf}
}

@inproceedings{bauerEqualityCheckingGeneral2020,
  title = {Equality {{Checking}} for {{General Type Theories}} in {{Andromeda}} 2},
  booktitle = {Mathematical {{Software}} – {{ICMS}} 2020},
  author = {Bauer, Andrej and Haselwarter, Philipp G. and Petković, Anja},
  editor = {Bigatti, Anna Maria and Carette, Jacques and Davenport, James H. and Joswig, Michael and family=Wolff, given=Timo, prefix=de, useprefix=true},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {253--259},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-52200-1_25},
  abstract = {We designed a user-extensible judgemental equality checking algorithm for general type theories that supports computation rules and extensionality rules. The user needs only provide the equality rules they wish to use, after which the algorithm devises an appropriate notion of normal form. The algorithm is a generalization of type-directed equality checking for Martin-Löf type theory, and we implemented it in the Andromeda~2 prover.},
  isbn = {978-3-030-52200-1},
  langid = {english},
  keywords = {Algorithmic equality checking,andromeda,Dependent type theory,Proof assistant},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Bauer et al/2020/Bauer et al_2020_Equality Checking for General Type Theories in Andromeda 2.pdf}
}

@article{boespflugMultilevelContextualType2011,
  title = {Multi-Level {{Contextual Type Theory}}},
  author = {Boespflug, Mathieu and Pientka, Brigitte},
  date = {2011-10-31},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  shortjournal = {Electron. Proc. Theor. Comput. Sci.},
  volume = {71},
  eprint = {1111.0087},
  eprinttype = {arxiv},
  pages = {29--43},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.71.3},
  url = {http://arxiv.org/abs/1111.0087},
  urldate = {2023-04-10},
  abstract = {Contextual type theory distinguishes between bound variables and meta-variables to write potentially incomplete terms in the presence of binders. It has found good use as a framework for concise explanations of higher-order unification, characterize holes in proofs, and in developing a foundation for programming with higher-order abstract syntax, as embodied by the programming and reasoning environment Beluga. However, to reason about these applications, we need to introduce meta\^2-variables to characterize the dependency on meta-variables and bound variables. In other words, we must go beyond a two-level system granting only bound variables and meta-variables. In this paper we generalize contextual type theory to n levels for arbitrary n, so as to obtain a formal system offering bound variables, meta-variables and so on all the way to meta\^n-variables. We obtain a uniform account by collapsing all these different kinds of variables into a single notion of variabe indexed by some level k. We give a decidable bi-directional type system which characterizes beta-eta-normal forms together with a generalized substitution operation.},
  keywords = {archived,Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Boespflug_Pientka/2011/Boespflug_Pientka_2011_Multi-level Contextual Type Theory.pdf}
}

@article{bradyIdrisGeneralpurposeDependently2013,
  title = {Idris, a General-Purpose Dependently Typed Programming Language: {{Design}} and Implementation},
  shorttitle = {Idris, a General-Purpose Dependently Typed Programming Language},
  author = {Brady, Edwin},
  date = {2013-09},
  journaltitle = {Journal of Functional Programming},
  volume = {23},
  number = {5},
  pages = {552--593},
  publisher = {{Cambridge University Press}},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S095679681300018X},
  url = {http://www.cambridge.org/core/journals/journal-of-functional-programming/article/idris-a-generalpurpose-dependently-typed-programming-language-design-and-implementation/418409138B4452969AC0736DB0A2C238},
  urldate = {2021-06-02},
  abstract = {Many components of a dependently typed programming language are by now well understood, for example, the underlying type theory, type checking, unification and evaluation. How to combine these components into a realistic and usable high-level language is, however, folklore, discovered anew by successive language implementors. In this paper, I describe the implementation of Idris, a new dependently typed functional programming language. Idris is intended to be a general-purpose programming language and as such provides high-level concepts such as implicit syntax, type classes and do notation. I describe the high-level language and the underlying type theory, and present a tactic-based method for elaborating concrete high-level syntax with implicit arguments and type classes into a fully explicit type theory. Furthermore, I show how this method facilitates the implementation of new high-level language constructs.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Brady/2013/Brady_2013_Idris, a general-purpose dependently typed programming language.pdf}
}

@article{changDependentTypeSystems2019,
  title = {Dependent Type Systems as Macros},
  author = {Chang, Stephen and Ballantyne, Michael and Turner, Milo and Bowman, William J.},
  date = {2019-12-20},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {4},
  pages = {3:1--3:29},
  doi = {10.1145/3371071},
  url = {http://doi.org/10.1145/3371071},
  urldate = {2021-06-28},
  abstract = {We present Turnstile+, a high-level, macros-based metaDSL for building dependently typed languages. With it, programmers may rapidly prototype and iterate on the design of new dependently typed features and extensions. Or they may create entirely new DSLs whose dependent type ``power'' is tailored to a specific domain. Our framework's support of language-oriented programming also makes it suitable for experimenting with systems of interacting components, e.g., a proof assistant and its companion DSLs. This paper explains the implementation details of Turnstile+, as well as how it may be used to create a wide-variety of dependently typed languages, from a lightweight one with indexed types, to a full spectrum proof assistant, complete with a tactic system and extensions for features like sized types and SMT interaction.},
  issue = {POPL},
  keywords = {dependent types,macros,proof assistants,type systems},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Chang et al/2019/Chang et al_2019_Dependent type systems as macros2.pdf}
}

@inproceedings{christiansenElaboratorReflectionExtending2016,
  title = {Elaborator Reflection: Extending {{Idris}} in {{Idris}}},
  shorttitle = {Elaborator Reflection},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Christiansen, David and Brady, Edwin},
  date = {2016-09-04},
  series = {{{ICFP}} 2016},
  pages = {284--297},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951932},
  url = {http://doi.org/10.1145/2951913.2951932},
  urldate = {2021-06-29},
  abstract = {Many programming languages and proof assistants are defined by elaboration from a high-level language with a great deal of implicit information to a highly explicit core language. In many advanced languages, these elaboration facilities contain powerful tools for program construction, but these tools are rarely designed to be repurposed by users. We describe elaborator reflection, a paradigm for metaprogramming in which the elaboration machinery is made directly available to metaprograms, as well as a concrete realization of elaborator reflection in Idris, a functional language with full dependent types. We demonstrate the applicability of Idris’s reflected elaboration framework to a number of realistic problems, we discuss the motivation for the specific features of its design, and we explore the broader meaning of elaborator reflection as it can relate to other languages.},
  isbn = {978-1-4503-4219-3},
  keywords = {dependent types,elaboration,Metaprogramming},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Christiansen_Brady/2016/Christiansen_Brady_2016_Elaborator reflection.pdf}
}

@article{cockxElaboratingDependentCo2020,
  title = {Elaborating Dependent (Co)Pattern Matching: {{No}} Pattern Left Behind},
  shorttitle = {Elaborating Dependent (Co)Pattern Matching},
  author = {Cockx, Jesper and Abel, Andreas},
  year = {2020/ed},
  journaltitle = {Journal of Functional Programming},
  volume = {30},
  pages = {e2},
  publisher = {{Cambridge University Press}},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796819000182},
  url = {http://www.cambridge.org/core/journals/journal-of-functional-programming/article/elaborating-dependent-copattern-matching-no-pattern-left-behind/F13CECDAB2B6200135D45452CA44A8B3},
  urldate = {2022-11-02},
  abstract = {In a dependently typed language, we can guarantee correctness of our programmes by providing formal proofs. To check them, the typechecker elaborates these programs and proofs into a low-level core language. However, this core language is by nature hard to understand by mere humans, so how can we know we proved the right thing? This question occurs in particular for dependent copattern matching, a powerful language construct for writing programmes and proofs by dependent case analysis and mixed induction/coinduction. A definition by copattern matching consists of a list of clauses that are elaborated to a case tree, which can be further translated to primitive eliminators. In previous work this second step has received a lot of attention, but the first step has been mostly ignored so far. We present an algorithm elaborating definitions by dependent copattern matching to a core language with inductive data types, coinductive record types, an identity type, and constants defined by well-typed case trees. To ensure correctness, we prove that elaboration preserves the first-match semantics of the user clauses. Based on this theoretical work, we reimplement the algorithm used by Agda to check left-hand sides of definitions by pattern matching. The new implementation is at the same time more general and less complex, and fixes a number of bugs and usability issues with the old version. Thus, we take another step towards the formally verified implementation of a practical dependently typed language.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Cockx_Abel/2020/Cockx_Abel_2020_Elaborating dependent (co)pattern matching.pdf}
}

@online{coqdevelopmentteamHowWritePlugins,
  title = {How to Write Plugins in {{Coq}}},
  author = {{Coq development team}},
  url = {https://github.com/coq/coq/tree/V8.15.2/doc/plugin_tutorial},
  urldate = {2022-07-15},
  abstract = {In addition to installing OCaml and Coq, you need to make sure that you also have the development headers for Coq, because you will need them to compile extensions. If you installed Coq from source or from OPAM, you already have the required headers. If you installed them from your system package manager, there may be a separate package which contains the development headers (for example, in Ubuntu they are contained in the package libcoq-ocaml-dev). It can help to install several tools for development.},
  langid = {english},
  organization = {{GitHub}}
}

@article{ebnerMetaprogrammingFrameworkFormal2017,
  title = {A Metaprogramming Framework for Formal Verification},
  author = {Ebner, Gabriel and Ullrich, Sebastian and Roesch, Jared and Avigad, Jeremy and family=Moura, given=Leonardo, prefix=de, useprefix=true},
  date = {2017-08-29},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {1},
  pages = {34:1--34:29},
  doi = {10.1145/3110278},
  url = {http://doi.org/10.1145/3110278},
  urldate = {2021-06-29},
  abstract = {We describe the metaprogramming framework currently used in Lean, an interactive theorem prover based on dependent type theory. This framework extends Lean's object language with an API to some of Lean's internal structures and procedures, and provides ways of reflecting object-level expressions into the metalanguage. We provide evidence to show that our implementation is performant, and that it provides a convenient and flexible way of writing not only small-scale interactive tactics, but also more substantial kinds of automation.},
  issue = {ICFP},
  keywords = {dependent type theory,metaprogramming,tactic language,theorem proving},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Ebner et al/2017/Ebner et al_2017_A metaprogramming framework for formal verification.pdf}
}

@thesis{eisenbergDependentTypesHaskell2017,
  title = {Dependent {{Types}} in {{Haskell}}: {{Theory}} and {{Practice}}},
  shorttitle = {Dependent {{Types}} in {{Haskell}}},
  author = {Eisenberg, Richard A.},
  date = {2017-08-12},
  eprint = {1610.07978},
  eprinttype = {arxiv},
  eprintclass = {cs},
  institution = {{University of Pennsylvania}},
  url = {http://arxiv.org/abs/1610.07978},
  urldate = {2022-08-10},
  abstract = {Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been adding new type-level programming features for some time. Many of these features---chiefly: generalized algebraic datatypes (GADTs), type families, kind polymorphism, and promoted datatypes---have brought Haskell to the doorstep of dependent types. Many dependently typed programs can even currently be encoded, but often the constructions are painful. In this dissertation, I describe Dependent Haskell, which supports full dependent types via a backward-compatible extension to today's Haskell. An important contribution of this work is an implementation, in GHC, of a portion of Dependent Haskell, with the rest to follow. The features I have implemented are already released, in GHC 8.0. This dissertation contains several practical examples of Dependent Haskell code, a full description of the differences between Dependent Haskell and today's Haskell, a novel type-safe dependently typed lambda-calculus (called Pico) suitable for use as an intermediate language for compiling Dependent Haskell, and a type inference and elaboration algorithm, Bake, that translates Dependent Haskell to type-correct Pico.},
  keywords = {Computer Science - Programming Languages},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Eisenberg/2017/Eisenberg_2017_Dependent Types in Haskell.pdf}
}

@inproceedings{ferreiraBidirectionalElaborationDependently2014,
  title = {Bidirectional {{Elaboration}} of {{Dependently Typed Programs}}},
  booktitle = {Proceedings of the 16th {{International Symposium}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  author = {Ferreira, Francisco and Pientka, Brigitte},
  date = {2014-09-08},
  series = {{{PPDP}} '14},
  pages = {161--174},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2643135.2643153},
  url = {http://doi.org/10.1145/2643135.2643153},
  urldate = {2021-02-15},
  abstract = {Dependently typed programming languages allow programmers to express a rich set of invariants and verify them statically via type checking. To make programming with dependent types practical, dependently typed systems provide a compact language for programmers where one can omit some arguments, called implicit, which can be inferred. This source language is then usually elaborated into a core language where type checking and fundamental properties such as normalization are well understood. Unfortunately, this elaboration is rarely specified and in general is ill-understood. This makes it not only difficult for programmers to understand why a given program fails to type check, but also is one of the reasons that implementing dependently typed programming systems remains a black art known only to a few. In this paper, we specify the design of a source language for a dependently typed programming language where we separate the language of programs from the language of types and terms occurring in types. Total functions in our language correspond directly to first-order inductive proofs over a specific index domain. We then give a bi-directional elaboration algorithm to translate source terms where implicit arguments can be omitted to a fully explicit core language and prove soundness of our elaboration. Our framework provides post-hoc explanation for elaboration found in the programming and proof environment, Beluga.},
  isbn = {978-1-4503-2947-7},
  keywords = {dependent types,type reconstruction},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Ferreira_Pientka/2014/Ferreira_Pientka_2014_Bidirectional Elaboration of Dependently Typed Programs.pdf}
}

@software{francescomazzoliTog2017,
  title = {Tog},
  author = {{Francesco Mazzoli}},
  date = {2017},
  url = {https://github.com/bitonic/tog},
  urldate = {2023-02-03},
  abstract = {A prototypical implementation of dependent types},
  version = {0.1.0}
}

@software{frisbyNfrisbyCoxswain2022,
  title = {Nfrisby/Coxswain},
  author = {Frisby, Nicolas},
  date = {2022-04-07T16:47:40Z},
  origdate = {2017-09-16T21:16:07Z; https://web.archive.org/web/20230317104356/https://github.com/nfrisby/coxswain},
  url = {https://github.com/nfrisby/coxswain},
  urldate = {2023-01-09},
  abstract = {A GHC type checker plugin for row types},
  keywords = {archived}
}

@report{gasterPolymorphicTypeSystem1996,
  type = {Technical Report},
  title = {A Polymorphic Type System for Extensible Records and Variants},
  author = {Gaster, Benedict R and Jones, Mark P},
  date = {1996},
  number = {NOTTCS-TR-96-3},
  institution = {{Department of Computer Science, University of Nottingham}},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Gaster_Jones/1996/Gaster_Jones_1996_A polymorphic type system for extensible records and variants.pdf}
}

@online{ghcdevelopmentteamGHCUserGuide,
  title = {{{GHC}} 9.4.2 {{User}}’s {{Guide}}},
  author = {{GHC development team}},
  url = {https://downloads.haskell.org/ghc/9.4.2/docs/users_guide/index.html},
  urldate = {2022-07-15}
}

@online{GHCPluginsResource,
  title = {{{GHC Plugins Resource Index}}},
  url = {https://mpickering.github.io/plugins.html},
  urldate = {2023-01-09}
}

@report{gonthierSmallScaleReflection2008,
  type = {report},
  title = {A {{Small Scale Reflection Extension}} for the {{Coq}} System},
  author = {Gonthier, Georges and Mahboubi, Assia and Tassi, Enrico},
  date = {2008},
  url = {https://hal.inria.fr/inria-00258384},
  urldate = {2022-07-22},
  abstract = {This document describes a set of extensions to the proof scripting language of the Coq proof assistant. While these extensions were developed to support a particular proof methodology - small-scale reflection - most of them actually are of a quite general nature, improving the functionality of Coq in basic areas such as script layout and structuring, proof context management, and rewriting. Consequently, and in spite of the title of this document, most of the extensions described here should be of interest for all Coq users, whether they embrace small-scale reflection or not.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Gonthier et al/2008/Gonthier et al_2008_A Small Scale Reflection Extension for the Coq system.pdf}
}

@inproceedings{gregoireCompiledImplementationStrong2002,
  title = {A Compiled Implementation of Strong Reduction},
  booktitle = {Proceedings of the Seventh {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Grégoire, Benjamin and Leroy, Xavier},
  date = {2002-09-17},
  series = {{{ICFP}} '02},
  pages = {235--246},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/581478.581501},
  url = {https://doi.org/10.1145/581478.581501},
  urldate = {2022-07-20},
  abstract = {Motivated by applications to proof assistants based on dependent types, we develop and prove correct a strong reducer and ß-equivalence checker for the λ-calculus with products, sums, and guarded fixpoints. Our approach is based on compilation to the bytecode of an abstract machine performing weak reductions on non-closed terms, derived with minimal modifications from the ZAM machine used in the Objective Caml bytecode interpreter, and complemented by a recursive "read back" procedure. An implementation in the Coq proof assistant demonstrates important speed-ups compared with the original interpreter-based implementation of strong reduction in Coq.},
  isbn = {978-1-58113-487-2},
  keywords = {abstract machine,beta-equivalence,calculus of constructions,Coq,normalization by evaluation,strong reduction,virtual machine},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Grégoire_Leroy/2002/Grégoire_Leroy_2002_A compiled implementation of strong reduction.pdf}
}

@misc{guidiImplementingTypeTheory2017,
  title = {Implementing {{Type Theory}} in {{Higher Order Constraint Logic Programming}}},
  author = {Guidi, Ferruccio and Coen, Claudio Sacerdoti and Tassi, Enrico},
  date = {2017-11-17},
  url = {https://hal.inria.fr/hal-01410567},
  urldate = {2021-03-24},
  abstract = {In this paper we are interested in high-level programming languages to implement the core components of an interactive theorem prover for a dependently typed language: the kernel — responsible for type-checking closed terms — and the elaborator — that manipulates terms with holes or, equivalently, partial proof terms. In the first part of the paper we confirm that λProlog, the language developed by Miller and Nadathur since the 80s, is extremely suitable for implementing the kernel, even when efficient techniques like reduction machines are employed. In the second part of the paper we turn our attention to the elaborator and we observe that the eager generative semantics inherited by Prolog makes it impossible to reason by induction over terms containing metavariables. We also conclude that the minimal extension to λProlog that allows to do so is the possibility to delay inductive predicates over flexible terms, turning them into (set of) constraints to be propagated according to user provided constraint propagation rules. Therefore we propose extensions to λProlog to declare and manipulate higher order constraints, and we implement the proposed extensions in the ELPI system. Our test case is the implementation of an elaborator for a type theory as a CLP extension to a kernel written in plain λProlog.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Guidi et al/2017/Guidi et al_2017_Implementing Type Theory in Higher Order Constraint Logic Programming.pdf}
}

@software{guillaumeallaisTypOS2022,
  title = {{{TypOS}}},
  author = {{Guillaume Allais} and {Malin Altenmüller} and {Conor McBride} and {Georgi Nakov} and {Fredrik Nordvall Forsberg} and {Craig Roy}},
  date = {2022-09-28T08:04:46Z},
  origdate = {2021-05-14T15:51:37Z},
  url = {https://github.com/msp-strath/TypOS},
  urldate = {2022-10-04},
  abstract = {being an operating system for typechecking processes},
  organization = {{MSP group}}
}

@unpublished{gundryTutorialImplementationDynamic,
  title = {A Tutorial Implementation of Dynamic Pattern Unification},
  author = {Gundry, Adam and McBride, Conor},
  abstract = {A higher-order unification algorithm is an essential component of a dependently typed programming language implementation, and understanding its capabilities is important if dependently typed programmers are to become productive. Miller showed that, for simply typed λ-terms in the pattern fragment (where metavariables are applied to spines of distinct bound variables), unification is decidable and most general unifiers exist. We describe an algorithm for pattern unification in a full-spectrum dependent type theory with dependent pairs (Σ-types). The algorithm exploits heterogeneous equality and a novel concept of ‘twin’ free variables to handle dependency. Moreover, it supports dynamic management of constraints, postponing equations that fall outside the pattern fragment in case other equations make them simpler. We aim to make sense both to language implementors and users, and to this end present our algorithm as a Haskell program.},
  howpublished = {Paper draft},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Gundry_McBride/undefined/Gundry_McBride_A tutorial implementation of dynamic pattern uniﬁcation.pdf}
}

@thesis{gundryTypeInferenceHaskell2013,
  type = {phdthesis},
  title = {Type {{Inference}}, {{Haskell}} and {{Dependent Types}}},
  author = {Gundry, Adam Michael},
  date = {2013},
  institution = {{University of Strathclyde}},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Gundry/2013/Gundry_2013_Type Inference, Haskell and Dependent Types.pdf}
}

@article{hallTypeClassesHaskell1996,
  title = {Type Classes in {{Haskell}}},
  author = {Hall, Cordelia V. and Hammond, Kevin and Peyton Jones, Simon L. and Wadler, Philip L.},
  date = {1996-03-01},
  journaltitle = {ACM Transactions on Programming Languages and Systems},
  shortjournal = {ACM Trans. Program. Lang. Syst.},
  volume = {18},
  number = {2},
  pages = {109--138},
  issn = {0164-0925},
  doi = {10.1145/227699.227700},
  abstract = {This article defines a set of type inference rules for resolving overloading introduced by type classes, as used in the functional programming language Haskell. Programs including type classes are transformed into ones which may be typed by standard Hindley-Milner inference rules. In contrast to other work on type classes, the rules presented here relate directly to Haskell programs. An innovative aspect of this work is the use of second-order lambda calculus to record type information in the transformed program.},
  keywords = {functional programming,Haskell,type classes,types},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Hall et al/1996/Hall et al_1996_Type classes in Haskell.pdf}
}

@online{henryModularizingGHC,
  title = {Modularizing {{GHC}}},
  author = {Henry, Sylvain and Ericson, John and Young, Jeffrey M},
  url = {https://hsyl20.fr/home/posts/2022-05-03-modularizing-ghc-paper.html},
  abstract = {GHC is the de facto main implementation of the Haskell programming language. Over its year history it has served well the needs of pure functional programmers and researchers alike. However, GHC is not exemplary of good large scale system design in a pure function language. Rather ironically, it violates the properties that draw people to functional programming in the first place: immutability, modularity, and composability. These scars have become more noticeable as modern projects currently underway, such as the Haskell Language Server and cross-compilation, aim to fulfill user needs and desires far more diverse than before. We believe a better GHC is possible. We write this paper to properly situate both the current state of GHC’s codebase and that better future state in the design space of large scale, pure, functional systems. Firstly, we document in detail, GHC’s architectural problems, such as low coherence and high coupling of mutable state, and their genesis. Secondly, we describe what we believe to be a superior design, drawing heavily on domain-driven design principles. Lastly, we sketch a plan to get this design implemented iteratively and durably, mentioning interactions with other ongoing refactorings (structured errors, Trees That Grow, etc.). All of this is informed not just by our own experience working on GHC and deep dives into its history, but also by the traditional software engineering literature. The paper is written from an engineering perspective, with the hope that our collection and recapitulation may provide insight into future best practices for other pure functional software engineers.},
  langid = {english},
  pubstate = {preprint},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Henry et al/undefined/Henry et al_Modularizing GHC.pdf}
}

@thesis{juanPracticalUnificationDependent2020,
  title = {Practical {{Unification}} for {{Dependent Type Checking}}},
  author = {Juan, Víctor López},
  date = {2020},
  institution = {{Chalmers University of Technology}},
  url = {https://research.chalmers.se/en/publication/519011},
  urldate = {2023-02-17},
  abstract = {When using popular dependently-typed languages such as Agda, Idris or Coq to write a proof or a program, some function arguments can be omitted, both to decrease code size and to improve readability. Type checking such a program involves inferring a combination of these implicit arguments that makes the program type-correct.{$<$}br /{$><$}br /{$>$}Finding such a combination of implicit arguments entails solving a higher-order unification problem.{$<$}br /{$>$}Because higher-order unification is undecidable, our aim is to infer the omitted arguments for as many programs as possible with a reasonable use of computational resources. The extent to which{$<$}br /{$>$}these goals are achieved affect how usable a dependently-typed proof assistant or programming language is in practice.{$<$}br /{$><$}br /{$>$}Current approaches to higher-order unification are in some cases too inflexible, postponing unification of terms until their types have been unified (Coq, Idris). In other cases they are too optimistic, which sometimes leads to ill-typed terms that break internal invariants (Agda).{$<$}br /{$><$}br /{$>$}In order to increase the flexibility of our unifier without sacrificing soundness, we use the twin types technique by Gundry and McBride. We simplify their approach so that it can be used within an existing type{$<$}br /{$>$}theory without changes to the syntax of terms. We also extend it so that it can handle more classes of constraints. We show that the resulting solutions are correct and unique.{$<$}br /{$><$}br /{$>$}Finally, we implement the resulting unification algorithm on an existing type checker prototype for a smaller variant of the Agda language, developed by Mazzoli and Abel. We make a suitable choice of internal term representation, and use few, if any, example-specific optimizations. We obtain a type-checker which avoids ill-typed solutions, and is also able to handle some challenging examples in time and memory comparable to the existing Agda implementation.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Juan/2020/Juan_2020_Practical Unification for Dependent Type Checking.pdf}
}

@article{koppelSearchingEntangledProgram2022,
  title = {Searching Entangled Program Spaces},
  author = {Koppel, James and Guo, Zheng and family=Vries, given=Edsko, prefix=de, useprefix=true and Solar-Lezama, Armando and Polikarpova, Nadia},
  date = {2022-08-31},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {6},
  pages = {91:23--91:51},
  doi = {10.1145/3547622},
  url = {https://doi.org/10.1145/3547622},
  urldate = {2022-12-05},
  abstract = {Many problem domains, including program synthesis and rewrite-based optimization, require searching astronomically large spaces of programs. Existing approaches often rely on building specialized data structures—version-space algebras, finite tree automata, or e-graphs—to compactly represent such spaces. At their core, all these data structures exploit independence of subterms; as a result, they cannot efficiently represent more complex program spaces, where the choices of subterms are entangled. We introduce equality-constrained tree automata (ECTAs), a new data structure, designed to compactly represent large spaces of programs with entangled subterms. We present efficient algorithms for extracting programs from ECTAs, implemented in a performant Haskell library, ecta. Using the ecta library, we construct Hectare, a type-driven program synthesizer for Haskell. Hectare significantly outperforms a state-of-the-art synthesizer Hoogle+—providing an average speedup of 8×—despite its implementation being an order of magnitude smaller.},
  issue = {ICFP},
  keywords = {e-graphs,Haskell,program synthesis,type systems},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Koppel et al/2022/Koppel et al_2022_Searching entangled program spaces.pdf}
}

@thesis{kuperLatticebasedDataStructures2015,
  type = {phdthesis},
  title = {Lattice-Based Data Structures for Deterministic Parallel and Distributed Programming},
  author = {Kuper, Lindsey},
  date = {2015},
  journaltitle = {ProQuest Dissertations and Theses},
  institution = {{Indiana University}},
  location = {{United States -- Indiana}},
  url = {https://www.proquest.com/docview/1723378413/abstract/C592471EAAE94DBFPQ/1},
  urldate = {2022-11-28},
  abstract = {Deterministic-by-construction parallel programming models guarantee that programs have the same observable behavior on every run, promising freedom from bugs caused by schedule nondeterminism. To make that guarantee, though, they must sharply restrict sharing of state between parallel tasks, usually either by disallowing sharing entirely or by restricting it to one type of data structure, such as single-assignment locations. I show that lattice-based data structures, or LVars, are the foundation for a guaranteed-deterministic parallel programming model that allows a more general form of sharing. LVars allow multiple assignments that are inflationary with respect to a given lattice. They ensure determinism by allowing only inflationary writes and "threshold" reads that block until a lower bound is reached. After presenting the basic LVars model, I extend it to support event handlers, which enable an event-driven programming style, and non-blocking "freezing" reads, resulting in a quasi-deterministic model in which programs behave deterministically modulo exceptions. I demonstrate the viability of the LVars model with LVish, a Haskell library that provides a collection of lattice-based data structures, a work-stealing scheduler, and a monad in which LVar computations run. LVish leverages Haskell's type system to index such computations with effect levels to ensure that only certain LVar effects can occur, hence statically enforcing determinism or quasi-determinism. I present two case studies of parallelizing existing programs using LVish: a k-CFA control flow analysis, and a bioinformatics application for comparing phylogenetic trees. Finally, I show how LVar-style threshold reads apply to the setting of convergent replicated data types (CvRDTs), which specify the behavior of eventually consistent replicated objects in a distributed system. I extend the CvRDT model to support deterministic, strongly consistent threshold queries. The technique generalizes to any lattice, and hence any CvRDT, and allows deterministic observations to be made of replicated objects before the replicas' states converge.},
  isbn = {9781339111544},
  langid = {english},
  pagetotal = {253},
  keywords = {Applied sciences,Data structures,Distributed programming,Lattice-based},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Kuper/2015/Kuper_2015_Lattice-based data structures for deterministic parallel and distributed.pdf}
}

@unpublished{leonardodemouraLeanMetaprogramming2021,
  title = {Lean 4 - Metaprogramming},
  author = {{Leonardo de Moura} and {Sebastian Ullrich}},
  date = {2021-01-06},
  url = {https://leanprover-community.github.io/lt2021/slides/leo-LT2021-meta.pdf},
  urldate = {2022-07-11},
  eventtitle = {Lean Together 2021},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Leonardo de Moura_Sebastian Ullrich/2021/Leonardo de Moura_Sebastian Ullrich_2021_Lean 4 - metaprogramming.pdf;/home/bohdan/Downloads/Dropbox/stuff/papers/Leonardo de Moura_Sebastian Ullrich/2021/Leonardo de Moura_Sebastian Ullrich_2021_Lean 4 - metaprogramming2.pdf}
}

@inproceedings{mahboubiCanonicalStructuresWorking2013,
  title = {Canonical Structures for the Working {{Coq}} User},
  booktitle = {Proceedings of the 4th International Conference on Interactive Theorem Proving},
  author = {Mahboubi, Assia and Tassi, Enrico},
  date = {2013},
  series = {{{ITP}}'13},
  pages = {19--34},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39634-2_5},
  url = {https://doi.org/10.1007/978-3-642-39634-2_5},
  abstract = {This paper provides a gentle introduction to the art of programming type inference with the mechanism of Canonical Structures. Programmable type inference has been one of the key ingredients for the successful formalization of the Odd Order Theorem using the Coq proof assistant. The paper concludes comparing the language of Canonical Structures to the one of Type Classes and Unification Hints.},
  isbn = {978-3-642-39633-5},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Mahboubi_Tassi/2013/Mahboubi_Tassi_2013_Canonical structures for the working coq user.pdf}
}

@thesis{malechaExtensibleProofEngineering2014,
  type = {phdthesis},
  title = {Extensible {{Proof Engineering}} in {{Intensional Type Theory}}},
  author = {Malecha, Gregory},
  date = {2014-11},
  institution = {{Harvard University, Graduate School of Arts \& Sciences.}},
  location = {{Cambridge, Massachusetts}},
  url = {http://nrs.harvard.edu/urn-3:HUL.InstRepos:17467172},
  abstract = {We increasingly rely on large, complex systems in our daily lives---from the computers that park our cars to the medical devices that regulate insulin levels to the servers that store our personal information in the cloud. As these systems grow, they become too complex for a person to understand, yet it is essential that they are correct. Proof assistants are tools that let us specify properties about complex systems and build, maintain, and check proofs of these properties in a rigorous way. Proof assistants achieve this level of rigor for a wide range of properties by requiring detailed certificates (proofs) that can be easily checked. In this dissertation, I describe a technique for compositionally building extensible automation within a foundational proof assistant for intensional type theory. My technique builds on computational reflection---where properties are checked by verified programs---which effectively bridges the gap between the low-level reasoning that is native to the proof assistant and the interesting, high-level properties of real systems. Building automation within a proof assistant provides a rigorous foundation that makes it possible to compose and extend the automation with other tools (including humans). However, previous approaches require using low-level proofs to compose different automation which limits scalability. My techniques allow for reasoning at a higher level about composing automation, which enables more scalable reflective reasoning. I demonstrate these techniques through a series of case studies centered around tasks in program verification.},
  pagetotal = {195},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Malecha/2014/Malecha_2014_Extensible Proof Engineering in Intensional Type Theory.pdf}
}

@online{mazzoliTypeCheckingUnification2016,
  title = {Type Checking through Unification},
  author = {Mazzoli, Francesco and Abel, Andreas},
  date = {2016-09-30},
  eprint = {1609.09709},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1609.09709},
  url = {http://arxiv.org/abs/1609.09709},
  urldate = {2023-02-09},
  abstract = {In this paper we describe how to leverage higher-order unification to type check a dependently typed language with meta-variables. The literature usually presents the unification algorithm as a standalone component, however the need to check definitional equality of terms while type checking gives rise to a tight interplay between type checking and unification. This interplay is a major source of complexity in the type-checking algorithm for existing dependently typed programming languages. We propose an algorithm that encodes a type-checking problem entirely in the form of unification constraints, reducing the complexity of the type-checking code by taking advantage of higher order unification, which is already part of the implementation of many dependently typed languages.},
  pubstate = {preprint},
  keywords = {Computer Science - Programming Languages},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Mazzoli_Abel/2016/Mazzoli_Abel_2016_Type checking through unification.pdf}
}

@incollection{mcbrideEpigramPracticalProgramming2005,
  title = {Epigram: {{Practical Programming}} with {{Dependent Types}}},
  shorttitle = {Epigram},
  booktitle = {Advanced {{Functional Programming}}},
  author = {McBride, Conor},
  editor = {Vene, Varmo and Uustalu, Tarmo},
  date = {2005},
  volume = {3622},
  pages = {130--170},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11546382_3},
  url = {https://link.springer.com/10.1007/11546382_3},
  urldate = {2022-12-06},
  isbn = {978-3-540-28540-3 978-3-540-31872-9},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/McBride/2005/McBride_2005_Epigram.pdf}
}

@article{millerLogicProgrammingLanguage1991,
  title = {A {{Logic Programming Language}} with {{Lambda-Abstraction}}, {{Function Variables}}, and {{Simple Unification}}},
  author = {Miller, Dale},
  date = {1991-09-01},
  journaltitle = {Journal of Logic and Computation},
  shortjournal = {Journal of Logic and Computation},
  volume = {1},
  number = {4},
  pages = {497--536},
  issn = {0955-792X},
  doi = {10.1093/logcom/1.4.497},
  url = {https://doi.org/10.1093/logcom/1.4.497},
  urldate = {2023-02-06},
  abstract = {It has been argued elsewhere that a logic programming language with function variables and λ-abstractions within terms makes a good meta-programming language, especially when an object-language contains notions of bound variables and scope. The λProlog logic programming language and the related Elf and Isabelle systems provide meta-programs with both function variables and λ-abstractions by containing implementations of higher order unification. This paper presents a logic programming language, called Lλ, that also contains both function variables and λ-abstractions, although certain restrictions are placed on occurrences of function variables. As a result of these restrictions, an implementation of Lλdoes not need to implement full higher-order unification. Instead, an extension to first-order unification that respects bound variable names and scopes is all that is required. Such unification problems are shown to be decidable and to possess most general unifiers when unifiers exist. A unification algorithm and logic programming interpreter are described and proved correct. Several examples of using Lλ as a meta-programming language are presented.},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Miller/1991/Miller_1991_A Logic Programming Language with Lambda-Abstraction, Function Variables, and.pdf}
}

@inproceedings{mouraLeanTheoremProver2021,
  title = {The {{Lean}} 4 {{Theorem Prover}} and {{Programming Language}}},
  booktitle = {Automated {{Deduction}} – {{CADE}} 28},
  author = {family=Moura, given=Leonardo, prefix=de, useprefix=false and Ullrich, Sebastian},
  editor = {Platzer, André and Sutcliffe, Geoff},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {625--635},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-79876-5_37},
  abstract = {Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.},
  isbn = {978-3-030-79876-5},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Moura_Ullrich/2021/Moura_Ullrich_2021_The Lean 4 Theorem Prover and Programming Language.pdf}
}

@incollection{n.g.debruijnPleaWeakerFrameworks1991,
  title = {A Plea for Weaker Frameworks},
  booktitle = {Logical Frameworks},
  author = {{N.G. de Bruijn}},
  editor = {{G. Huet} and {G. Plotkin}},
  date = {1991},
  pages = {40--67},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  isbn = {978-0-521-41300-8}
}

@article{najdTreesThatGrow2017,
  title = {Trees That Grow},
  author = {Najd, Shayan and Peyton Jones, Simon},
  date = {2017-01-01},
  journaltitle = {Journal of Universal Computer Science (JUCS)},
  volume = {23},
  pages = {47--62},
  url = {https://www.microsoft.com/en-us/research/publication/trees-that-grow/},
  urldate = {2022-07-12},
  abstract = {We study the notion of extensibility in functional data types, as a new approach to the problem of decorating abstract syntax trees with additional information. We observed the need for such extensibility while redesigning the data types representing Haskell abstract syntax inside Glasgow Haskell Compiler (GHC). Specifically, we describe a programming idiom that exploits type-level […]},
  langid = {american},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Najd_Jones/2017/Najd_Jones_2017_Trees that grow.pdf}
}

@article{nanevskiContextualModalType2008,
  title = {Contextual Modal Type Theory},
  author = {Nanevski, Aleksandar and Pfenning, Frank and Pientka, Brigitte},
  date = {2008-06},
  journaltitle = {ACM Transactions on Computational Logic},
  shortjournal = {ACM Trans. Comput. Logic},
  volume = {9},
  number = {3},
  pages = {1--49},
  issn = {1529-3785, 1557-945X},
  doi = {10.1145/1352582.1352591},
  url = {https://dl.acm.org/doi/10.1145/1352582.1352591},
  urldate = {2022-09-15},
  abstract = {The intuitionistic modal logic of necessity is based on the judgmental notion of categorical truth. In this article we investigate the consequences of relativizing these concepts to explicitly specified contexts. We obtain contextual modal logic and its type-theoretic analogue. Contextual modal type theory provides an elegant, uniform foundation for understanding metavariables and explicit substitutions. We sketch some applications in functional programming and logical frameworks.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Nanevski et al/2008/Nanevski et al_2008_Contextual modal type theory.pdf}
}

@thesis{norellPracticalProgrammingLanguage2007,
  type = {phdthesis},
  title = {Towards a Practical Programming Language Based on Dependent Type Theory},
  author = {Norell, Ulf},
  date = {2007},
  institution = {{Chalmers University of Technology and Göteborg University}},
  location = {{Göteborg, Sweden}},
  url = {https://www.cse.chalmers.se/~ulfn/papers/thesis.pdf},
  abstract = {Dependent type theories [ML72] have a long history of being used for theorem proving. One aspect of type theory which makes it very powerful as a proof language is that it mixes deduction with computation. This also makes type theory a good candidate for programming—the strength of the type system allows properties of programs to be stated and established, and the computational properties provide semantics for the programs.},
  langid = {english},
  pagetotal = {166},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Norell/2007/Norell_2007_Towards a practical programming language based on dependent type theory.pdf}
}

@article{oderskyTypeInferenceConstrained1999,
  title = {Type Inference with Constrained Types},
  author = {Odersky, Martin and Sulzmann, Martin and Wehr, Martin},
  date = {1999},
  journaltitle = {Theory and Practice of Object Systems},
  volume = {5},
  number = {1},
  pages = {35--55},
  issn = {1096-9942},
  doi = {10.1002/(SICI)1096-9942(199901/03)5:1<35::AID-TAPO4>3.0.CO;2-4},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-9942%28199901/03%295%3A1%3C35%3A%3AAID-TAPO4%3E3.0.CO%3B2-4},
  urldate = {2022-12-13},
  abstract = {We present a general framework HM(X) for type systems with constraints. The framework stays in the tradition of the Hindley/Milner type system. Its type system instances are sound under a standard untyped compositional semantics. We can give a generic type inference algorithm for HM(X) so that, under sufficient conditions on X, type inference will always compute the principal type of a term. We discuss instances of the framework that deal with polymorphic records, equational theories, and subtypes. © 1999 John Wiley \& Sons, Inc.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Odersky et al/1999/Odersky et al_1999_Type inference with constrained types.pdf}
}

@inproceedings{orchardHaskellTypeConstraints2010a,
  title = {Haskell Type Constraints Unleashed},
  booktitle = {Proceedings of the 10th International Conference on {{Functional}} and {{Logic Programming}}},
  author = {Orchard, Dominic and Schrijvers, Tom},
  date = {2010-04-19},
  series = {{{FLOPS}}'10},
  pages = {56--71},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12251-4_6},
  url = {https://doi.org/10.1007/978-3-642-12251-4_6},
  urldate = {2022-12-13},
  abstract = {The popular Glasgow Haskell Compiler extends the Haskell 98 type system with several powerful features, leading to an expressive language of type terms. In contrast, constraints over types have received much less attention, creating an imbalance in the expressivity of the type system. In this paper, we rectify the imbalance, transferring familiar type-level constructs, synonyms and families, to the language of constraints, providing a symmetrical set of features at the type-level and constraint-level. We introduce constraint synonyms and constraint families, and illustrate their increased expressivity for improving the utility of polymorphic EDSLs in Haskell, amongst other examples. We provide a discussion of the semantics of the new features relative to existing type system features and similar proposals, including details of termination.},
  isbn = {978-3-642-12250-7},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Orchard_Schrijvers/2010/Orchard_Schrijvers_2010_Haskell type constraints unleashed.pdf}
}

@inproceedings{pangPluggingHaskell2004,
  title = {Plugging {{Haskell}} In},
  booktitle = {Proceedings of the 2004 {{ACM SIGPLAN}} Workshop on {{Haskell}}},
  author = {Pang, André and Stewart, Don and Seefried, Sean and Chakravarty, Manuel M. T.},
  date = {2004-09-22},
  series = {Haskell '04},
  pages = {10--21},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1017472.1017478},
  url = {http://doi.org/10.1145/1017472.1017478},
  urldate = {2022-09-14},
  abstract = {Extension languages enable users to expand the functionality of an application without touching its source code. Commonly, these languages are dynamically typed languages, such as Lisp, Python, or domain-specific languages, which support runtime plugins via dynamic loading of components. We show that Haskell can be comfortably used as a statically typed extension language for both Haskell and foreign-language applications supported by the Haskell FFI, and that it can perform type-safe dynamic loading of plugins using dynamic types. Moreover, we discuss how plugin support is especially useful to applications where Haskell is used as an embedded domain-specific language (EDSL). We explain how to realise type-safe plugins using dynamic types, runtime compilation, and dynamic linking, exploiting infrastructure provided by the Glasgow Haskell Compiler. We demonstrate the practicability of our approach with several applications that serve as running examples.},
  isbn = {978-1-58113-850-4},
  keywords = {dynamic loading,dynamic typing,extension languages,functional programming,plugins,staged type inference},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Pang et al/2004/Pang et al_2004_Plugging Haskell in.pdf}
}

@inproceedings{pedrotLtac2TacticalWarfare2019,
  title = {Ltac2: {{Tactical Warfare}}},
  author = {Pédrot, Pierre-Marie},
  date = {2019},
  pages = {3},
  location = {{Cascais, Portugal}},
  url = {https://popl19.sigplan.org/details/CoqPL-2019/8/Ltac2-Tactical-Warfare},
  abstract = {We present Ltac2, a proposal for the replacement of the Ltac tactic language that is shipped with Coq as the default interface to build up proofs interactively. Ltac2 is primarily motivated by two antagonistic desires, namely extending the expressivity and regularity of the historical tactic language of Coq while maintaining a strong backward compatibility. We thereafter give a bird’s eye view of the features and semantics of the current state of Ltac2.},
  eventtitle = {The {{Fifth International Workshop}} on {{Coq}} for {{Programming Languages}} /},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Pédrot/2019/Pédrot_2019_Ltac2.pdf}
}

@online{PerformanceRegressionIssue,
  title = {Performance Regression · {{Issue}} \#3435 · Agda/Agda},
  url = {https://github.com/agda/agda/issues/3435},
  urldate = {2022-12-05},
  abstract = {I have some code that type-checks about ten times slower using Agda 2.6.0-4a5e1dc, compared to Agda 2.5.4.2: git clone http://www.cse.chalmers.se/\textasciitilde nad/repos/equality/ (cd equality \&amp;\&amp; git ch...},
  langid = {english},
  organization = {{GitHub}}
}

@article{peytonjonesPracticalTypeInference2007,
  title = {Practical Type Inference for Arbitrary-Rank Types},
  author = {Peyton Jones, Simon and Vytiniotis, Dimitrios and Weirich, Stephanie and Shields, Mark},
  date = {2007-01},
  journaltitle = {Journal of Functional Programming},
  volume = {17},
  number = {1},
  pages = {1--82},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796806006034},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/practical-type-inference-for-arbitraryrank-types/5339FB9DAB968768874D4C20FA6F8CB6},
  urldate = {2022-06-09},
  abstract = {Haskell's popularity has driven the need for ever more expressive type system features, most of which threaten the decidability and practicality of Damas-Milner type inference. One such feature is the ability to write functions with higher-rank types – that is, functions that take polymorphic functions as their arguments. Complete type inference is known to be undecidable for higher-rank (impredicative) type systems, but in practice programmers are more than willing to add type annotations to guide the type inference engine, and to document their code. However, the choice of just what annotations are required, and what changes are required in the type system and its inference algorithm, has been an ongoing topic of research. We take as our starting point a λ-calculus proposed by Odersky and Läufer. Their system supports arbitrary-rank polymorphism through the exploitation of type annotations on λ-bound arguments and arbitrary sub-terms. Though elegant, and more convenient than some other proposals, Odersky and Läufer's system requires many annotations. We show how to use local type inference (invented by Pierce and Turner) to greatly reduce the annotation burden, to the point where higher-rank types become eminently usable. Higher-rank types have a very modest impact on type inference. We substantiate this claim in a very concrete way, by presenting a complete type-inference engine, written in Haskell, for a traditional Damas-Milner type system, and then showing how to extend it for higher-rank types. We write the type-inference engine using a monadic framework: it turns out to be a particularly compelling example of monads in action. The paper is long, but is strongly tutorial in style. Although we use Haskell as our example source language, and our implementation language, much of our work is directly applicable to any ML-like functional language.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Jones et al/2007/Jones et al_2007_Practical type inference for arbitrary-rank types.pdf}
}

@unpublished{peytonjonesTypeInferenceConstraint2019,
  title = {Type Inference as Constraint Solving: How {{GHC}}’s Type Inference Engine Actually Works},
  shorttitle = {Type Inference as Constraint Solving},
  author = {Peyton Jones, Simon},
  date = {2019-06-15},
  url = {https://www.microsoft.com/en-us/research/publication/type-inference-as-constraint-solving-how-ghcs-type-inference-engine-actually-works/},
  urldate = {2022-06-28},
  abstract = {The Haskell compiler GHC includes a type inference engine for a rather sophisticated type system.~ You might worry that a complicated type system leads to a very complicated type inference engine. ~ You’d be right, but we have learned a lot about how to structure type inference so that the complexity does not get out […]},
  langid = {american},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Jones/2019/Jones_2019_Type inference as constraint solving.pdf}
}

@inproceedings{pientkaOptimizingHigherOrderPattern2003,
  title = {Optimizing {{Higher-Order Pattern Unification}}},
  booktitle = {Automated {{Deduction}} – {{CADE-19}}},
  author = {Pientka, Brigitte and Pfenning, Frank},
  editor = {Baader, Franz},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {473--487},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45085-6_40},
  abstract = {We present an abstract view of existential variables in a dependently typed lambda-calculus based on modal type theory. This allows us to justify optimizations to pattern unification such as linearization, which eliminates many unnecessary occurs-checks. The presented modal framework explains a number of features of the current implementation of higher-order unification in Twelf and provides insight into several optimizations. Experimental results demonstrate significant performance improvement in many example applications of Twelf, including those in the area of proof-carrying code.},
  isbn = {978-3-540-45085-6},
  langid = {english},
  keywords = {Logic Programming,Modal Logic,Modal Variable,Normal Object,Sequent Calculus},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Pientka_Pfenning/2003/Pientka_Pfenning_2003_Optimizing Higher-Order Pattern Unification.pdf}
}

@inproceedings{prottEmbeddingFunctionalLogic2023,
  title = {Embedding {{Functional Logic Programming}} in~{{Haskell}} via~a~{{Compiler Plugin}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {Prott, Kai-Oliver and Teegen, Finn and Christiansen, Jan},
  editor = {Hanus, Michael and Inclezan, Daniela},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {37--55},
  publisher = {{Springer Nature Switzerland}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-24841-2_3},
  abstract = {We present a technique to embed a functional logic language in Haskell using a GHC plugin. Our approach is based on a monadic lifting that models the functional logic semantics explicitly. Using a GHC plugin, we get many language extensions that GHC provides for free in the embedded language. As a result, we obtain a seamless embedding of a functional logic language, without having to implement a full compiler. We briefly show that our approach can be used to embed other domain-specific languages as well. Furthermore, we can use such a plugin to build a full blown compiler for our language.},
  isbn = {978-3-031-24841-2},
  langid = {english},
  keywords = {DSL,Functional programming,GHC plugin,Haskell,Logic programming,Monadic transformation},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Prott et al/2023/Prott et al_2023_Embedding Functional Logic Programming in Haskell via a Compiler Plugin.pdf}
}

@online{ptoopTypeInferenceThought2022,
  type = {Tweet},
  title = {Type Inference Is a Thought Trap. {{Write}} More Types and Fewer Programs.},
  author = {{PTOOP}},
  date = {2022-02-07},
  url = {https://twitter.com/PTOOP/status/1490496253276340227},
  urldate = {2022-12-06},
  langid = {english},
  organization = {{Twitter}},
  annotation = {archived at https://web.archive.org/web/20220207012758/https://twitter.com/PTOOP/status/1490496253276340227}
}

@article{reedHybridMetalogicalFramework,
  title = {A {{Hybrid Metalogical Framework}}},
  author = {Reed, Jason},
  abstract = {The methodology by which deductive systems and metatheorems about them are encoded in the logical framework LF is well understood. Many (but not all) of these ideas have already been successfully extended to the case of encoding stateful deductive systems in the linear logical framework LLF.},
  langid = {english},
  file = {/home/bohdan/Zotero/storage/N9ARXKFG/Reed - A Hybrid Metalogical Framework.pdf}
}

@article{ringerQEDLargeSurvey2019,
  title = {{{QED}} at {{Large}}: {{A Survey}} of {{Engineering}} of {{Formally Verified Software}}},
  shorttitle = {{{QED}} at {{Large}}},
  author = {Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary},
  date = {2019},
  journaltitle = {Foundations and Trends® in Programming Languages},
  shortjournal = {FNT in Programming Languages},
  volume = {5},
  number = {2-3},
  eprint = {2003.06458},
  eprinttype = {arxiv},
  pages = {102--281},
  issn = {2325-1107, 2325-1131},
  doi = {10.1561/2500000045},
  url = {http://arxiv.org/abs/2003.06458},
  urldate = {2020-11-12},
  abstract = {Development of formal proofs of correctness of programs can increase actual and perceived reliability and facilitate better understanding of program specifications and their underlying assumptions. Tools supporting such development have been available for over 40 years, but have only recently seen wide practical use. Projects based on construction of machine-checked formal proofs are now reaching an unprecedented scale, comparable to large software projects, which leads to new challenges in proof development and maintenance. Despite its increasing importance, the field of proof engineering is seldom considered in its own right; related theories, techniques, and tools span many fields and venues. This survey of the literature presents a holistic understanding of proof engineering for program correctness, covering impact in practice, foundations, proof automation, proof organization, and practical proof development.},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,D.2.4,F.3.1,I.2.3},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Ringer et al/2019/Ringer et al_2019_QED at Large.pdf}
}

@online{robertestelleHeavyCouplingHaskell2019,
  title = {Heavy Coupling of {{Haskell}} Source Modules · {{Issue}} \#3512 · Agda/Agda},
  author = {{Robert Estelle}},
  date = {2019-01-20},
  url = {https://github.com/agda/agda/issues/3512},
  urldate = {2022-07-21},
  abstract = {The modules under src/full are currently closely interdependent which makes reasoning/learning about Agda's compiler implementation somewhat challenging and makes refactoring difficult.},
  organization = {{GitHub, Agda issues}}
}

@unpublished{selsamTabledTypeclassResolution2020,
  title = {Tabled {{Typeclass Resolution}}},
  author = {Selsam, Daniel and Ullrich, Sebastian and family=Moura, given=Leonardo, prefix=de, useprefix=true},
  date = {2020-01-21},
  eprint = {2001.04301},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.04301},
  url = {http://arxiv.org/abs/2001.04301},
  urldate = {2022-06-07},
  abstract = {Typeclasses provide an elegant and effective way of managing ad-hoc polymorphism in both programming languages and interactive proof assistants. However, the increasingly sophisticated uses of typeclasses within proof assistants, especially within Lean's burgeoning mathematics library, mathlib, have elevated once-theoretical limitations of existing typeclass resolution procedures into major impediments to ongoing progress. The two most devastating limitations of existing procedures are exponential running times in the presence of diamonds and divergence in the presence of cycles. We present a new procedure, tabled typeclass resolution, that solves both problems by tabling, which is a generalization of memoizing originally introduced to address similar limitations of early logic programming systems. We have implemented our procedure for the upcoming version (v4) of Lean, and have confirmed empirically that our implementation is exponentially faster than existing systems in the presence of diamonds. Although tabling is notoriously difficult to implement, our procedure is notably lightweight and could easily be implemented in other systems. We hope our new procedure facilitates even more sophisticated uses of typeclasses in both software development and interactive theorem proving.},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Selsam et al/2020/Selsam et al_2020_Tabled Typeclass Resolution.pdf}
}

@article{serranoQuickLookImpredicativity2020,
  title = {A Quick Look at Impredicativity},
  author = {Serrano, Alejandro and Hage, Jurriaan and Peyton Jones, Simon and Vytiniotis, Dimitrios},
  date = {2020-08-02},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {4},
  pages = {89:1--89:29},
  doi = {10.1145/3408971},
  url = {https://doi.org/10.1145/3408971},
  urldate = {2022-07-04},
  abstract = {Type inference for parametric polymorphism is wildly successful, but has always suffered from an embarrassing flaw: polymorphic types are themselves not first class. We present Quick Look, a practical, implemented, and deployable design for impredicative type inference. To demonstrate our claims, we have modified GHC, a production-quality Haskell compiler, to support impredicativity. The changes required are modest, localised, and are fully compatible with GHC's myriad other type system extensions.},
  issue = {ICFP},
  keywords = {constraint-based inference,impredicative polymorphism,Type systems},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Serrano et al/2020/Serrano et al_2020_A quick look at impredicativity.pdf}
}

@thesis{spiwackVerifiedComputingHomological2011,
  type = {phdthesis},
  title = {Verified {{Computing}} in {{Homological Algebra}}},
  author = {Spiwack, Arnaud},
  date = {2011-03-25},
  institution = {{Ecole Polytechnique X}},
  url = {https://pastel.archives-ouvertes.fr/pastel-00605836},
  urldate = {2022-10-20},
  abstract = {The object of this thesis is the study of the ability of the Coq system to mix proofs and programs in practice. Our approach consists in implementing part of the program Kenzo, a computer algebra tool for homological algebra under some constraint. We want to be able to read the program as a proof with a computational content, these proofs much compute efficiently, and we try to avoid duplication of proofs or part thereof. We show, first, how the requirement of efficiency leads to revise some aspects of traditional mathematics. We propose a suitable categorical abstraction, both for clarity and to avoid duplications. This abstraction, though different from what is customary in mathematics, allow to formulate the constructs of homological algebra in a style much like that of Kenzo. We propose, then, modifications to the Coq programm. A first one to make proofs more convenient, by allowing the use of more fine grain tactics which are often necessary when dependent types are common. The second modification to leverage the arithmetical abilities of the processor to compute more efficiently on integers. Finally, we propose some leads to improve both sharing and clarity of the proofs. Unfortunately, they push the system beyond its limits. Hence, we show that Coq is not always up to its promises and that theoretical works will be necessary to understand how these limits can be relaxed.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Spiwack/2011/Spiwack_2011_Verified Computing in Homological Algebra.pdf}
}

@thesis{stewartDynamicExtensionTyped2010,
  type = {Thesis},
  title = {Dynamic Extension of Typed Functional Languages},
  author = {Stewart, Don},
  date = {2010},
  institution = {{UNSW Sydney}},
  doi = {10.26190/unsworks/23404},
  url = {http://hdl.handle.net/1959.4/50220},
  urldate = {2022-09-14},
  abstract = {We present a solution to the problem of dynamic extension in statically typed functional languages with type erasure. The presented solution retains the benefits of static checking, including type safety, aggressive optimizations, and native code compilation of components, while allowing extensibility of programs at runtime. Our approach is based on a framework for dynamic extension in a statically typed setting, combining dynamic linking, runtime type checking, first class modules and code hot swapping. We show that this framework is sufficient to allow a broad class of dynamic extension capabilities in any statically typed functional language with type erasure semantics. Uniquely, we employ the full compile-time type system to perform runtime type checking of dynamic components, and emphasize the use of native code extension to ensure that the performance benefits of static typing are retained in a dynamic environment. We also develop the concept of fully dynamic software architectures, where the static core is minimal and all code is hot swappable. Benefits of the approach include hot swappable code and sophisticated application extension via embedded domain specific languages. We instantiate the concepts of the framework via a full implementation in the Haskell programming language: providing rich mechanisms for dynamic linking, loading, hot swapping, and runtime type checking in Haskell for the first time. We demonstrate the feasibility of this architecture through a number of novel applications: an extensible text editor; a plugin-based network chat bot; a simulator for polymer chemistry; and xmonad, an extensible window manager. In doing so, we demonstrate that static typing is no barrier to dynamic extension.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Stewart/2010/Stewart_2010_Dynamic extension of typed functional languages.pdf}
}

@inproceedings{stuckeyTypeProcessingConstraint2006,
  title = {Type {{Processing}} by {{Constraint Reasoning}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Stuckey, Peter J. and Sulzmann, Martin and Wazny, Jeremy},
  editor = {Kobayashi, Naoki},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--25},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11924661_1},
  abstract = {Herbrand constraint solving or unification has long been understood as an efficient mechanism for type checking and inference for programs using Hindley/Milner types. If we step back from the particular solving mechanisms used for Hindley/Milner types, and understand type operations in terms of constraints we not only give a basis for handling Hindley/Milner extensions, but also gain insight into type reasoning even on pure Hindley/Milner types, particularly for type errors. In this paper we consider typing problems as constraint problems and show which constraint algorithms are required to support various typing questions. We use a light weight constraint reasoning formalism, Constraint Handling Rules, to generate suitable algorithms for many popular extensions to Hindley/Milner types. The algorithms we discuss are all implemented as part of the freely available Chameleon system.},
  isbn = {978-3-540-48938-2},
  langid = {english},
  keywords = {Functional Dependency,Type Class,Type Error,Type Processing,Typing Problem},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Stuckey et al/2006/Stuckey et al_2006_Type Processing by Constraint Reasoning.pdf}
}

@article{swierstraDataTypesCarte2008,
  title = {Data Types à La Carte},
  author = {Swierstra, Wouter},
  date = {2008-07},
  journaltitle = {Journal of Functional Programming},
  volume = {18},
  number = {4},
  pages = {423--436},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796808006758},
  abstract = {This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell's monolithic IO monad.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Swierstra/2008/Swierstra_2008_Data types à la carte.pdf}
}

@article{tassiBiDirectionalRefinementAlgorithm2012,
  title = {A {{Bi-Directional Refinement Algorithm}} for the {{Calculus}} of ({{Co}}){{Inductive Constructions}}},
  author = {Tassi, Enrico and Coen, Claudio Sacerdoti and Ricciotti, Wilmer and Asperti, Andrea},
  date = {2012-03-02},
  journaltitle = {Logical Methods in Computer Science},
  volume = {Volume 8, Issue 1},
  publisher = {{Episciences.org}},
  doi = {10.2168/LMCS-8(1:18)2012},
  url = {https://lmcs.episciences.org/1044/pdf},
  urldate = {2022-05-23},
  abstract = {The paper describes the refinement algorithm for the Calculus of (Co)Inductive Constructions (CIC) implemented in the interactive theorem prover Matita. The refinement algorithm is in charge of giving a meaning to the terms, types and proof terms directly written by the user or generated by using tactics, decision procedures or general automation. The terms are written in an "external syntax" meant to be user friendly that allows omission of information, untyped binders and a certain liberal use of user defined sub-typing. The refiner modifies the terms to obtain related well typed terms in the internal syntax understood by the kernel of the ITP. In particular, it acts as a type inference algorithm when all the binders are untyped. The proposed algorithm is bi-directional: given a term in external syntax and a type expected for the term, it propagates as much typing information as possible towards the leaves of the term. Traditional mono-directional algorithms, instead, proceed in a bottom-up way by inferring the type of a sub-term and comparing (unifying) it with the type expected by its context only at the end. We propose some novel bi-directional rules for CIC that are particularly effective. Among the benefits of bi-directionality we have better error message reporting and better inference of dependent types. Moreover, thanks to bi-directionality, the coercion system for sub-typing is more effective and type inference generates simpler unification problems that are more likely to be solved by the inherently incomplete higher order unification algorithms implemented. Finally we introduce in the external syntax the notion of vector of placeholders that enables to omit at once an arbitrary number of arguments. Vectors of placeholders allow a trivial implementation of implicit arguments and greatly simplify the implementation of primitive and simple tactics.},
  keywords = {matita},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Tassi et al/2012/Tassi et al_2012_A Bi-Directional Refinement Algorithm for the Calculus of (Co)Inductive.pdf}
}

@software{teamCoqProofAssistant2022,
  title = {The {{Coq Proof Assistant}}, Version 8.15.0},
  author = {Team, The Coq Development},
  date = {2022-01-13},
  doi = {10.5281/zenodo.5846982},
  url = {https://zenodo.org/record/5846982},
  urldate = {2022-07-20},
  abstract = {Coq is a formal proof management system. It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs. Typical applications include the certification of properties of programming languages (e.g. the CompCert compiler certification project, the Verified Software Toolchain for verification of C programs, or the Iris framework for concurrent separation logic), the formalization of mathematics (e.g. the full formalization of the Feit-Thompson theorem, or homotopy type theory), and teaching. Coq version 8.15 integrates many bug fixes, deprecations and cleanups as well as a few new features. We highlight some of the most impactful changes here: The apply with tactic no longer renames arguments unless compatibility flag Apply With Renaming is set. Improvements to the auto tactic family, fixing the Hint Unfold behavior, and generalizing the use of discrimination nets. The typeclasses eauto tactic has a new best\_effort option allowing it to return partial solutions to a proof search problem, depending on the mode declarations associated to each constraint. This mode is used by typeclass resolution during type inference to provide more precise error messages. Many commands and options were deprecated or removed after deprecation and more consistently support locality attributes. The Import command is extended with import\_categories to select the components of a module to import or not, including features such as hints, coercions, and notations. A visual Ltac debugger is now available in CoqIDE. See the Changes in 8.15.0 section below for the detailed list of changes, including potentially breaking changes marked with Changed. Coq's reference manual for 8.15, documentation of the 8.15 standard library and developer documentation of the 8.15 ML API are also available. Emilio Jesús Gallego Arias, Gaëtan Gilbert, Michael Soegtrop and Théo Zimmermann worked on maintaining and improving the continuous integration system and package building infrastructure. Erik Martin-Dorel has maintained the Coq Docker images that are used in many Coq projects for continuous integration. The OPAM repository for Coq packages has been maintained by Guillaume Claret, Karl Palmskog, Matthieu Sozeau and Enrico Tassi with contributions from many users. A list of packages is available at https://coq.inria.fr/opam/www/. The Coq Platform has been maintained by Michael Soegtrop and Enrico Tassi. Our current maintainers are Yves Bertot, Frédéric Besson, Ali Caglayan, Tej Chajed, Cyril Cohen, Pierre Corbineau, Pierre Courtieu, Maxime Dénès, Jim Fehrle, Julien Forest, Emilio Jesús Gallego Arias, Gaëtan Gilbert, Georges Gonthier, Benjamin Grégoire, Jason Gross, Hugo Herbelin, Vincent Laporte, Olivier Laurent, Assia Mahboubi, Kenji Maillard, Guillaume Melquiond, Pierre-Marie Pédrot, Clément Pit-Claudel, Pierre Roux, Kazuhiko Sakaguchi, Vincent Semeria, Michael Soegtrop, Arnaud Spiwack, Matthieu Sozeau, Enrico Tassi, Laurent Théry, Anton Trunov, Li-yao Xia and Théo Zimmermann. See the Coq Team face book page for more details. The 41 contributors to this version are Tanaka Akira, Frédéric Besson, Juan Conejero, Ali Caglayan, Cyril Cohen, Adrian Dapprich, Maxime Dénès, Stéphane Desarzens, Christian Doczkal, Andrej Dudenhefner, Jim Fehrle, Emilio Jesús Gallego Arias, Attila Gáspár, Gaëtan Gilbert, Jason Gross, Hugo Herbelin, Jasper Hugunin, Bart Jacobs, Ralf Jung, Grant Jurgensen, Jan-Oliver Kaiser, Wojciech Karpiel, Fabian Kunze, Olivier Laurent, Yishuai Li, Erik Martin-Dorel, Guillaume Melquiond, Jean-Francois Monin, Pierre-Marie Pédrot, Rudy Peterson, Clément Pit-Claudel, Seth Poulsen, Pierre Roux, Takafumi Saikawa, Kazuhiko Sakaguchi, Michael Soegtrop, Matthieu Sozeau, Enrico Tassi, Laurent Théry, Anton Trunov and Théo Zimmerman. The Coq community at large helped improve the design of this new version via the GitHub issue and pull request system, the Coq development mailing list coqdev@inria.fr, the coq-club@inria.fr mailing list, the Discourse forum and the Coq Zulip chat. Version 8.15's development spanned 3 months from the release of Coq 8.14.0. Gaëtan Gilbert is the release manager of Coq 8.15. This release is the result of 384 merged PRs, closing 67 issues.},
  organization = {{Zenodo}},
  keywords = {formal proofs,mathematical software,proof assistant},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Team/2022/Team_2022_The Coq Proof Assistant.pdf}
}

@article{tejiscakDependentlyTypedCalculus2020,
  title = {A Dependently Typed Calculus with Pattern Matching and Erasure Inference},
  author = {Tejiščák, Matúš},
  date = {2020-08-02},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {4},
  pages = {91:1--91:29},
  doi = {10.1145/3408973},
  url = {https://doi.org/10.1145/3408973},
  urldate = {2021-02-01},
  abstract = {Some parts of dependently typed programs constitute evidence of their type-correctness and, once checked, are unnecessary for execution. These parts can easily become asymptotically larger than the remaining runtime-useful computation, which can cause normally linear-time programs run in exponential time, or worse. We should not make programs run slower by just describing them more precisely. Current dependently typed systems do not erase such computation satisfactorily. By modelling erasure indirectly through type universes or irrelevance, they impose the limitations of these means to erasure. Some useless computation then cannot be erased and idiomatic programs remain asymptotically sub-optimal. In this paper, we explain why we need erasure, that it is different from other concepts like irrelevance, and propose a dependently typed calculus with pattern matching with erasure annotations to model it. We show that erasure in well-typed programs is sound in that it commutes with reduction. Assuming the Church-Rosser property, erasure furthermore preserves convertibility in general. We also give an erasure inference algorithm for erasure-unannotated or partially annotated programs and prove it sound, complete, and optimal with respect to the typing rules of the calculus. Finally, we show that this erasure method is effective in that it can not only recover the expected asymptotic complexity in compiled programs at run time, but it can also shorten compilation times.},
  issue = {ICFP},
  keywords = {dependent types,erasure,inference},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Tejiščák/2020/Tejiščák_2020_A dependently typed calculus with pattern matching and erasure inference.pdf}
}

@misc{theagdateamAgdaUserManual2022,
  title = {Agda {{User Manual}}},
  author = {{The Agda Team}},
  date = {2022-09-30},
  url = {https://agda.readthedocs.io/_/downloads/en/v2.6.2.2/pdf/},
  urldate = {2022-10-04}
}

@misc{theagdateamAgdaUserManual2023,
  title = {Agda {{User Manual}}},
  author = {{The Agda Team}},
  date = {2023-01-30},
  url = {https://agda.readthedocs.io/_/downloads/en/v2.6.3/pdf/},
  urldate = {2023-05-22},
  keywords = {archived},
  annotation = {https://web.archive.org/web/20230522091202/https://agda.readthedocs.io/\_/downloads/en/v2.6.3/pdf/},
  file = {/home/bohdan/Zotero/storage/WZT3LAMI/pdf.pdf}
}

@misc{theidristeamIdrisTutorial2021,
  title = {The {{Idris}} Tutorial},
  author = {{The Idris Team}},
  date = {2021-10-23},
  url = {https://docs.idris-lang.org/_/downloads/en/v1.3.4/pdf/},
  urldate = {2022-04-10},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/The Idris Team/2021/The Idris Team_2021_The Idris tutorial.pdf}
}

@inproceedings{ullrichNotationsHygienicMacro2020,
  ids = {ullrichNotationsHygienicMacro2020a},
  title = {Beyond {{Notations}}: {{Hygienic Macro Expansion}} for {{Theorem Proving Languages}}},
  shorttitle = {Beyond {{Notations}}},
  booktitle = {Automated {{Reasoning}}},
  author = {Ullrich, Sebastian and family=Moura, given=Leonardo, prefix=de, useprefix=true},
  editor = {Peltier, Nicolas and Sofronie-Stokkermans, Viorica},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {167--182},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-51054-1_10},
  abstract = {In interactive theorem provers (ITPs), extensible syntax is not only crucial to lower the cognitive burden of manipulating complex mathematical objects, but plays a critical role in developing reusable abstractions in libraries. Most ITPs support such extensions in the form of restrictive “syntax sugar” substitutions and other ad hoc mechanisms, which are too rudimentary to support many desirable abstractions. As a result, libraries are littered with unnecessary redundancy. Tactic languages in these systems are plagued by a seemingly unrelated issue: accidental name capture, which often produces unexpected and counterintuitive behavior. We take ideas from the Scheme family of programming languages and solve these two problems simultaneously by proposing a novel hygienic macro system custom-built for ITPs. We further describe how our approach can be extended to cover type-directed macro expansion resulting in a single, uniform system offering multiple abstraction levels that range from supporting simplest syntax sugars to elaboration of formerly baked-in syntax. We have implemented our new macro system and integrated it into the upcoming version (v4) of the Lean theorem prover. Despite its expressivity, the macro system is simple enough that it can easily be integrated into other systems.},
  isbn = {978-3-030-51054-1},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Ullrich_de Moura/2020/Ullrich_de Moura_2020_Beyond Notations.pdf}
}

@article{ullrichUnchainedEmbracingLocal2021,
  title = {`do' {{Unchained}}: {{Embracing Local Imperativity}} in a {{Purely Functional Language}} ({{Functional Pearl}})},
  author = {Ullrich, Sebastian and Moura, Leonardo De},
  date = {2021-08},
  journaltitle = {Proc. ACM Program. Lang},
  volume = {6},
  pages = {28},
  doi = {10.1145/3547640},
  url = {https://leanprover.github.io/papers/do.pdf},
  abstract = {Purely functional programming languages pride themselves with reifying effects that are implicit in imperative languages into reusable and composable abstractions such as monads. This reification allows for more exact control over effects as well as the introduction of new or derived effects. However, despite libraries of more and more powerful abstractions over effectful operations being developed, syntactically the common do notation still lags behind equivalent imperative code it is supposed to mimic regarding verbosity and code duplication. In this paper, we explore extending do notation with other imperative language features that can be added to simplify monadic code: local mutation, early return, and iteration. We present formal translation rules that compile these features back down to purely functional code, show that the generated code can still be reasoned over using an implementation of the translation in the Lean 4 theorem prover, and formally prove the correctness of the translation rules relative to a simple static and dynamic semantics in Lean.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Ullrich_Moura/2021/Ullrich_Moura_2021_`do' Unchained.pdf}
}

@thesis{victorlopezjuanPracticalHeterogeneousUnification2021,
  title = {Practical {{Heterogeneous Unification}} for {{Dependent Type Checking}}},
  author = {{Víctor López Juan}},
  date = {2021},
  institution = {{Chalmers University of Technology}},
  url = {https://research.chalmers.se/en/publication/527051},
  urldate = {2023-02-03},
  abstract = {Dependent types can specify in detail which inputs to a program are allowed, and how the properties of its output depend on the inputs. A program called the type checker assesses whether a program has a given type, thus detecting situations where the implementation of a program potentially differs from its intended behaviour. When using dependent types, the inputs to a program often occur in the types of other inputs or in the type of the output. The user may omit some of these redundant inputs when calling the program, expecting the type-checker to infer those subterms automatically. {$<$}br /{$><$}br /{$>$}Some type-checkers restrict the inference of missing subterms to those cases where there is a provably unique solution. This makes the process more predictable, but also limits the situations in which the omitted terms can be inferred; specially when considering that whether a unique solution exists is in general an undecidable problem. This restriction can be made less limiting by giving flexibility to the type-checker regarding the order in which the missing subterms are inferred. The type-checker can then use the information gained by filling in any one subterm in order to infer others, until the whole program has been type-checked. However, this flexibility may in some cases lead to ill-typed subterms being inferred, breaking internal invariants of the type-checker and causing it to crash or loop. The type checker could mitigate this by consistently rechecking the type of each inferred subterm, but this might incur a performance penalty.{$<$}br /{$>$} {$<$}br /{$>$}An approach by Gundry and McBride (2012) called twin types has the potential to afford the desired flexibility while preserving well-typedness invariants. However, this method had not yet been tested in a practical setting. In this thesis we streamline the method of twin types in order to ease its practical implementation, justify the correctness of our modifications, and then implement the result in an established dependently-typed language called Agda. We show that our implementation resolves certain existing bugs in Agda while still allowing a wide range of examples to be type-checked, and achieves this without heavily impacting performance.},
  isbn = {9789179055837},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Víctor López Juan/2021/Víctor López Juan_2021_Practical Heterogeneous Unification for Dependent Type Checking.pdf}
}

@software{victorlopezjuanTog2020,
  title = {{{Tog}}⁺},
  author = {{Víctor López Juan}},
  date = {2020-08-28},
  url = {https://framagit.org/vlopez/togt},
  urldate = {2023-02-08},
  abstract = {Tog⁺ (pronounced tog-tee) is right now a laboratory to experiment in implementing dependent types. It is a variant/extension of tog.},
  version = {1.0}
}

@article{vytiniotisOutsideInModularType2011,
  ids = {vytiniotisOutsideInModularType2011a},
  title = {{{OutsideIn}}({{X}}) {{Modular}} Type Inference with Local Assumptions},
  author = {Vytiniotis, Dimitrios and Jones, Simon and Schrijvers, Tom and Sulzmann, Martin},
  date = {2011-09},
  journaltitle = {Journal of Functional Programming},
  volume = {21},
  number = {4-5},
  pages = {333--412},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796811000098},
  url = {http://www.cambridge.org/core/journals/journal-of-functional-programming/article/outsideinx-modular-type-inference-with-local-assumptions/65110D74CF75563F91F9C68010604329},
  urldate = {2021-06-15},
  abstract = {Advanced type system features, such as GADTs, type classes and type families, have proven to be invaluable language extensions for ensuring data invariants and program correctness. Unfortunately, they pose a tough problem for type inference when they are used as local type assumptions. Local type assumptions often result in the lack of principal types and cast the generalisation of local let-bindings prohibitively difficult to implement and specify. User-declared axioms only make this situation worse. In this paper, we explain the problems and – perhaps controversially – argue for abandoning local let-binding generalisation. We give empirical results that local let generalisation is only sporadically used by Haskell programmers. Moving on, we present a novel constraint-based type inference approach for local type assumptions. Our system, called OutsideIn(X), is parameterised over the particular underlying constraint domain X, in the same way as HM(X). This stratification allows us to use a common metatheory and inference algorithm. OutsideIn(X) extends the constraints of X by introducing implication constraints on top. We describe the strategy for solving these implication constraints, which, in turn, relies on a constraint solver for X. We characterise the properties of the constraint solver for X so that the resulting algorithm only accepts programs with principal types, even when the type system specification accepts programs that do not enjoy principal types. Going beyond the general framework, we give a particular constraint solver for X = type classes + GADTs + type families, a non-trivial challenge in its own right. This constraint solver has been implemented and distributed as part of GHC 7.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Vytiniotis et al/2011/Vytiniotis et al_2011_OutsideIn(X) Modular type inference with local assumptions.pdf}
}

@unpublished{weirichImplementingDependentTypes2022,
  title = {Implementing {{Dependent Types}} in Pi-Forall},
  author = {Weirich, Stephanie},
  date = {2022-08-15},
  location = {{OPLSS 2022}},
  url = {https://raw.githubusercontent.com/sweirich/pi-forall/2022/doc/oplss.pdf},
  howpublished = {Lecture notes},
  langid = {english},
  pagetotal = {54},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Weirich/2022/Weirich_2022_Implementing Dependent Types in pi-forall.pdf}
}

@article{zilianiComprehensibleGuideNew2017,
  title = {A Comprehensible Guide to a New Unifier for {{CIC}} Including Universe Polymorphism and Overloading},
  author = {Ziliani, Beta and Sozeau, Matthieu},
  date = {2017},
  journaltitle = {Journal of Functional Programming},
  volume = {27},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796817000028},
  url = {https://www.cambridge.org/core/product/identifier/S0956796817000028/type/journal_article},
  urldate = {2020-08-01},
  abstract = {Unification is a core component of every proof assistant or programming language featuring dependent types. In many cases, it must deal with higher-order problems up to conversion. Since unification in such conditions is undecidable, unification algorithms may include several heuristics to solve common problems. However, when the stack of heuristics grows large, the result and complexity of the algorithm can become unpredictable.},
  langid = {english},
  file = {/home/bohdan/Downloads/Dropbox/stuff/papers/Ziliani_Sozeau/2017/Ziliani_Sozeau_2017_A comprehensible guide to a new unifier for CIC including universe polymorphism.pdf;/home/bohdan/Downloads/Dropbox/stuff/papers/Ziliani_Sozeau/2017/Ziliani_Sozeau_2017_A comprehensible guide to a new unifier for CIC including universe polymorphism2.pdf}
}
