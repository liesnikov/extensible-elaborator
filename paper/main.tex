% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  sigconf,
  screen,
  review]{acmart}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\widowpenalty10000
\clubpenalty10000
\author{Bohdan Liesnikov}
\orcid{0009-0000-2216-8830}
\affiliation{
  \institution{Delft University of Technology}
  \city{Delft}
  \country{The Netherlands}}
\email{B.Liesnikov@tudelft.nl}
\author{Jesper Cockx}
\orcid{0000-0003-3862-4073}
\affiliation{
  \institution{Delft University of Technology}
  \city{Delft}
  \country{The Netherlands}}
\email{J.G.H.Cockx@tudelft.nl}
\begin{abstract}
Dependently typed languages such as Agda, Coq, Lean, and Idris are used for statically enforcing properties of programs and for enabling type-driven development.
While there has been a lot of work to find the right theoretical foundations for the core type theory underlying these languages, their implementations have been studied to a smaller extent.
In particular, theoretical works rarely consider the plethora of features that exist in big languages like Agda.
As a consequence the developers have to make a lot of adaptations in the implementations, which leads to codebases that are hard to maintain.
In this work we focus on a part of the type-checker called the elaborator that transforms user-friendly surface syntax to a small well-behaved core theory.
Our core idea is to use an open datatype for constraints and plugin system for solvers.
This allows for a more compact base elaborator implementation while enabling extensions to the type system.
\end{abstract}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={ExEl: Building an Elaborator Using Extensible Constraints},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{ExEl: Building an Elaborator Using Extensible Constraints}
\author{}
\date{\today}

\begin{document}
\maketitle

\hypertarget{sec:introduction}{%
\section{Introduction}\label{sec:introduction}}

Strongly-typed languages allow us to catch certain classes of bugs at
compile-time by checking the implementation against the type-signature.
When the types are provided by the user they can be viewed as a form of
specification, constraining the behaviour of the programs. This comes
with the benefit of more static guarantees but with an increased toll on
the user to supply more precise information about the program. Many
languages choose to infer types, but another option is to use the
information in the types to infer parts of the program. This idea was
aptly worded by Conor McBride as ``Write more types and fewer
programs.''
\citep[chap.~2.1]{ptoopTypeInferenceThought2022, mcbrideEpigramPracticalProgramming2005}
Some examples of this include overloaded functions in Java, implicits in
Scala, and type classes in Haskell.

In dependently typed languages like Agda
\citep{norellPracticalProgrammingLanguage2007, theagdateamAgdaUserManual2023a},
Coq \citep{thecoqdevelopmentteamCoqProofAssistant2022} or Idris
\citep{bradyIdrisGeneralpurposeDependently2013} the types can be much
more precise. This gives us an opportunity to infer even larger parts of
the program from the type in the process of elaboration. Examples
include implicit arguments in Agda, implicit coercions in Coq, and
tactic arguments in Idris. The inference or ``solving'' can be not only
automatic but also interactive or partially automatic. Holes in Agda and
proof obligations in Coq are examples of interactive, while Canonical
Structures \citep{mahboubiCanonicalStructuresWorking2013} in Coq and
program-synthesis for holes in Haskell
\citep{koppelSearchingEntangledProgram2022} are partially automatic. All
these mechanisms use different algorithms and have various degrees of
extensibility. These are typically not very well isolated from each
other and can therefore interact in unexpected ways, for example in case
between implicits and instance search
\citep{agdausersPerformanceRegressionIssue2018}. This puts a toll on the
language developer to modulate the interactions and for the user to
understand these features.

In all of these examples, solvers evolved organically over time together
with the language. Coq historically struggled with similar issues in the
elaborator: for example, Canonical Structures was not properly
documented for 15 years \citep{mahboubiCanonicalStructuresWorking2013}.
Agda experimented with features baked into the core of the type system
-- like sized types -- which brought their own solver infrastructure
\citep{abelExtensionMartinLofType2016}. Lean 4 is aiming to allow the
users to develop new surface-level features
\citep{leonardodemouraLeanMetaprogramming2021} using elaboration monads
\citep{mouraLeanTheoremProver2021}, somewhat akin to elaborator
reflection in Idris
\citep{christiansenElaboratorReflectionExtending2016}, but Lean 3 was
built in a more conventional way \citep{demouraLeanTheoremProver2015a}.

One common piece of infrastructure needed by these inference algorithms
are metavariables, also known as ``existential variables''
\citep[chap.~2.2.1]{thecoqdevelopmentteamCoqProofAssistant2022}, which
represent as-of-yet unknown parts of the program. Another one is the
need is to constrain two terms to be equal, known as unification.
Metavariables and unification are heavily used throughout the compiler,
for inference of implicit arguments and for general type-checking,
making the compiler sensitive towards changes in unification algorithms.
Because of the complexity unification breaking changes are often
discovered only when ran against a large existing project on CI, like
\texttt{cubical} or \texttt{stdlib} for Agda or \texttt{unimath} for
Coq.

In this paper we propose a new architecture for an extensible elaborator
for dependently typed languages. The idea is to provide an API for
developers to tap into the elaboration procedure with custom solvers
that can manipulate metavariables and constraints placed on them. This
design separates the `what' the solvers are doing from the `when',
making the interaction points between different parts of the
type-checker explicit. As a result, this allows the developer to reason
more easily about exceptions and asynchronicity in the type-checker and
add new features in a more contained fashion. Practically, this means
that each feature is contained within one module, as opposed to being
spread around the codebase.

Contributions:

\begin{itemize}
\tightlist
\item
  We propose a new design blueprint for implementing a language that is
  extensible with new constraints and new solvers. It supports type
  classes (Section \ref{sec:case-typeclasses}), implicit arguments
  (Section \ref{sec:case-implicits}), implicit coercions and tactic
  arguments (Section \ref{sec:coercion-tactics}).
\item
  We propose a view on metavariables as communication channels for the
  solvers, drawing parallels with asynchronous programming primitives
  (Section \ref{sec:solvers-implementation}).
\item
  We decompose the usual components of a type-checker, like the unifier
  in Agda, into a suite of solvers which can be extended and interleaved
  by user-provided plugins (Section
  \ref{sec:constraints_and_unification}).
\item
  Following the blueprint, we present a prototype implementation of a
  dependently typed language available at
  \href{https://github.com/liesnikov/extensible-elaborator}{github.com/liesnikov/extensible-elaborator}.
\end{itemize}

\hypertarget{sec:unification_constraint_based_elaboration_and_design_challanges}{%
\section{Unification, constraint-based elaboration and design
challenges}\label{sec:unification_constraint_based_elaboration_and_design_challanges}}

Constraints have been an integral part of compilers for strongly-typed
languages for a while \citep{oderskyTypeInferenceConstrained1999}. For
example, both Haskell \citep{vytiniotisOutsideInModularType2011} and
Agda \citep[ chap.~3]{norellPracticalProgrammingLanguage2007} use
constraints extensively. In the former case, they are even reflected and
can be manipulated by the user
\citep[chap.~6.10.3]{orchardHaskellTypeConstraints2010a, ghcdevelopmentteamGHCUserGuide2022}.
This has proved to be a good design decision for GHC, as is reflected,
for example in a talk by \citet{peytonjonesTypeInferenceConstraint2019},
as well as in a few published sources
\citep{vytiniotisOutsideInModularType2011, peytonjonesPracticalTypeInference2007}.

In the land of dependently typed languages constraints are much less
principled. Agda has a family of constraints\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Monad/Base.hs\#L1157-L1191}{./src/full/Agda/TypeChecking/Monad/Base.hs\#L1157-L1191}.
  Here and hencefoth we shorten the links in footnotes to paths in the
  repository, the source code can be found at
  \href{https://github.com/agda/agda/blob/v2.6.4/}{github.com/agda/agda/blob/v2.6.4/}.}
that grew organically, currently counting 19 constructors. Idris
technically has constraints\footnote{\href{https://github.com/idris-lang/Idris2/blob/e673d05a67b82591131e35ccd50fc234fb9aed85/src/Core/UnifyState.idr}{./src/Core/UnifyState.idr}
  at
  \href{https://github.com/idris-lang/Idris2/blob/e673d05a67b82591131e35ccd50fc234fb9aed85}{github.com/idris-lang/Idris2/blob/e673d0}},
with the only two constructors being equality constraints for two terms
and for two sequences of terms. The same\footnote{\href{https://github.com/leanprover/lean4/blob/0a031fc9bbb43c274bb400f121b13711e803f56c/src/Lean/Meta/Match/Basic.lean\#L161}{./src/Lean/Meta/Match/Basic.lean\#L161}
  at
  \href{https://github.com/leanprover/lean4/blob/0a031fc9bbb43c274bb400f121b13711e803f56c/}{github.com/leanprover/lean4/blob/0a031f}}
holds for Lean. These languages either use constraints in a restricted,
single use-case manner -- namely, for unification -- or in an ad-hoc
manner.

In this section we will demonstrate why a more methodical approach to
constraints will result in more robust elaborators across the board. We
go over three typical challenges that come up when building a compiler
for a dependently typed language and the way they are usually solved. We
cover unification of the base language and the complexity of managing
the state of the unifier in Section \ref{sec:conversion_checking}. Then
we take a look at different kinds of implicit arguments and their
implementation in Section \ref{sec:implicit-arguments}. We briefly touch
on the problem of extending the unifier in Section
\ref{sec:extending-unification}. Finally, we summarize the issues in
Section \ref{sec:summary-of-the-issues}.

\hypertarget{sec:conversion_checking}{%
\subsection{Unification in presence of
meta-variables}\label{sec:conversion_checking}}

As mentioned in the introduction, in the process of type-checking we use
unification to compare terms, which is notoriously hard to implement.
The complexity stems from the desire of compiler writers to implement
the most powerful unifier, while being limited by the fact that
higher-order unification is undecidable in general. Some of this
complexity is unavoidable, but we can manage it better by splitting up
the unifier into smaller modular parts. In practice, this means that one
does not have to fit together an always-growing unifier but can instead
write different cases separately.

An example from Agda's elaborator is type-driven unification, which is
spread between about a hundred functions and 2200 lines of
code\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Conversion.hs}{./src/full/Agda/TypeChecking/Conversion.hs}}.
Each of the functions implements part of the ``business logic'' of the
unifier. But all of them contain a lot of code that deals with
bookkeeping related to metavariables and constraints:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They have to throw and catch exceptions, driving the control flow of
  the unification.
\item
  They have to compute blockers that determine when a postponed
  constraint is retried.
\item
  They have to deal with cases where either or both of the sides
  equation or its type are either metavariables or terms whose
  evaluation is blocked on some metavariables.
\end{enumerate}

Concretely, this code uses functions like \texttt{noConstraints} and
\texttt{dontAssignMetas} which rely on specific behaviour of the
constraint solver system. Other functions like \texttt{abortIfBlocked},
\texttt{reduce} and \texttt{catchConstraint}/\texttt{patternViolation}
force the programmer to make a choice between letting the constraint
system handle blockers or doing it manually. These things are known to
be brittle and pose an increased mental overhead when making changes.

The unifier is heavily used throughout the type-checker: either as
function calls to \texttt{leqType} when type-checking terms and
\texttt{compareType} when type-checking applications, or as raised
constraints \texttt{ValueCmp} and \texttt{SortCmp} from
\texttt{equalTerm} while checking applications or definitions,
\texttt{ValueCmpOnFace} from \texttt{equalTermOnFace} again while
checking applications. At the same it is unintuitive and full of
intricacies as indicated by multiple comments\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Conversion.hs\#L484-L485}{./src/full/Agda/TypeChecking/Conversion.hs\#\#L484-L485},
  \href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Conversion.hs\#L541-L549}{./src/full/Agda/TypeChecking/Conversion.hs\#L541-L549}}.

We would like the compiler-writer to separate the concerns of managing
constraints and blockers from the actual logic of the comparison
function. In fact, if we zoom in on the \texttt{compareAtom} function,
the core can be expressed in about 20 lines\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Conversion.hs\#L550-L599}{./src/full/Agda/TypeChecking/Conversion.hs\#L550-L599}}
of simplified code, stripping out size checks, cumulativity, polarity,
and forcing. This is precisely what we would like the developer to
write.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{case}\NormalTok{ (m, n) }\KeywordTok{of}
\NormalTok{  (}\DataTypeTok{Lit}\NormalTok{ l1, }\DataTypeTok{Lit}\NormalTok{ l2) }\OperatorTok{|}\NormalTok{ l1 }\OperatorTok{==}\NormalTok{ l2 }\OtherTok{{-}\textgreater{}} \FunctionTok{return}\NormalTok{ ()}
\NormalTok{  (}\DataTypeTok{Var}\NormalTok{ i es, }\DataTypeTok{Var}\NormalTok{ i\textquotesingle{} es\textquotesingle{}) }\OperatorTok{|}\NormalTok{ i }\OperatorTok{==}\NormalTok{ i\textquotesingle{} }\OtherTok{{-}\textgreater{}} \KeywordTok{do}
\NormalTok{      a }\OtherTok{\textless{}{-}}\NormalTok{ typeOfBV i}
\NormalTok{      compareElims [] [] a (var i) es es\textquotesingle{}}
\NormalTok{  (}\DataTypeTok{Con}\NormalTok{ x ci xArgs, }\DataTypeTok{Con}\NormalTok{ y \_ yArgs) }\OperatorTok{|}\NormalTok{ x }\OperatorTok{==}\NormalTok{ y }\OtherTok{{-}\textgreater{}} \KeywordTok{do}
\NormalTok{      t\textquotesingle{} }\OtherTok{\textless{}{-}}\NormalTok{ conType x t}
\NormalTok{      compareElims t\textquotesingle{} (}\DataTypeTok{Con}\NormalTok{ x ci []) xArgs yArgs}
  \OperatorTok{...}
\end{Highlighting}
\end{Shaded}

The functions described above are specific to Agda but in other major
languages we can find similar problems with unifiers being large modules
that are hard to understand. The sizes of modules with unifiers are as
follows: Idris (1.5kloc\footnote{\href{https://github.com/idris-lang/Idris2/blob/102d7ebc18a9e881021ed4b05186cccda5274cbe/src/Core/Unify.idr}{./src/Core/Unify.idr}}),
Lean (1.8kloc\footnote{\href{https://github.com/leanprover/lean4/blob/75252d2b85df8cb9231020a556a70f6d736e7ee5/src/Lean/Meta/ExprDefEq.lean}{./src/Lean/Meta/ExprDefEq.lean}}),
Coq (1.8kloc\footnote{\href{https://github.com/coq/coq/blob/b35c06c3ab3ed4911311b4a9428a749658d3eff1/pretyping/evarconv.mli}{./pretyping/evarconv.mli}
  at
  \href{github.com/coq/coq/blob/b35c06c3ab3ed4911311b4a9428a749658d3eff1/}{github.com/coq/coq/blob/b35c06}}).
For Haskell, which is not a dependently typed language yet, but does
have a constraints system
\citep{peytonjonesTypeInferenceConstraint2019}, this number is at
2kloc\footnote{\href{https://gitlab.haskell.org/ghc/ghc/-/blob/b81cd709df8054b8b98ac05d3b9affcee9a8b840/compiler/GHC/Core/Unify.hs}{./compiler/GHC/Core/Unify.hs}
  at
  \href{https://gitlab.haskell.org/ghc/ghc/-/blob/b81cd709df8054b8b98ac05d3b9affcee9a8b840}{gitlab.haskell.org/ghc/ghc/-/blob/b81cd709d}}.

\hypertarget{sec:implicit-arguments}{%
\subsection{Type-checking function application in presence of implicit
arguments}\label{sec:implicit-arguments}}

During the type-checking of function application there may be different
kinds of implicit arguments to infer, for example, instance arguments,
or tactic arguments. If we start from a simple case of type-checking an
application of a function symbol to regular arguments, every next
extension needs to be handled as a special case.

Take Agda -- when checking an application of implicit
arguments\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Implicit.hs\#L96-L128}{./src/full/Agda/TypeChecking/Implicit.hs\#L96-L128}}
we already have to carry the information on how the argument will be
resolved and then create a specific kind of metavariable\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Implicit.hs\#L130-L149}{./src/full/Agda/TypeChecking/Implicit.hs\#L130-L149}}
\citep[chap.~3]{norellPracticalProgrammingLanguage2007} for each of
those cases.

For handling \texttt{auto} variables, Idris 2
\citep[chap.~13.1]{theidristeamIdrisTutorial2021} has to essentially
inline the search procedure through a chain of elaboration function
calls (\texttt{checkApp} to \texttt{checkAppWith} to
\texttt{checkAppWith\textquotesingle{}}) to
\texttt{makeAutoImplicit}\footnote{\href{https://github.com/idris-lang/Idris2/blob/870bc824371d504a03af937f326216302210a875/src/TTImp/Elab/App.idr\#L224-L241}{./src/TTImp/Elab/App.idr\#L224-L241}}.
This can accommodate interfaces (or type classes), but one can imagine
that if a different kind of implicit was added, like tactic arguments or
Canonical Structures, we would have to inline the search again,
requiring a non-trivial modification to the elaboration mechanism.

While the codebases above show that it is certainly possible to extend
languages with new features if the language was not written with
extensibility in mind, this can lead to rather ad hoc solutions.

\hypertarget{sec:extending-unification}{%
\subsection{Extending unification}\label{sec:extending-unification}}

While writing a unifier is hard enough as it is, at times the developers
might want to give their users ability to extend the unification
procedure.

Canonical Structures
\citep{saibiOutilsGeneriquesModelisation1999, mahboubiCanonicalStructuresWorking2013}
was already mentioned as it is in the overlap between type classes and
unification hints. Adding it to a language that does not support them
requires an extension of the unification algorithm with a rule that says
that projection from a canonical structure is an injective function
\citep[eq. 1]{mahboubiCanonicalStructuresWorking2013}.

One could also provide means to do so manually in a more general case,
by allowing users to declare certain symbols as injective. This is one
of the features requested by the Agda users
\citep{agdausersInjectiveUnificationPragma2023}.

Another example of this can be adding rules of associativity and
commutativity to the unifier, as described in the thesis by
\citet{holtenDependentTypeCheckingModulo2023}. It required 2000 lines of
code\footnote{\href{https://github.com/LHolten/agda-commassoc/tree/defenitional-commutativity}{github.com/LHolten/agda-commassoc/tree/defenitional-commutativity}}.
We would like to make changes such as this more feasible.

\hypertarget{sec:summary-of-the-issues}{%
\subsection{Summary of the issues}\label{sec:summary-of-the-issues}}

The examples above show that when building a dependently typed language
the core might be perfectly elegant and simple, but the features that
appear on top of it complicate the design.

One can also observe that while the code in existing implementations
might rely on constraints, the design at large does not put them at the
centre of the picture and instead views them primarily as a gadget.
Agda's constraint solver\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Constraints.hs\#L247-L298}{src/full/Agda/TypeChecking/Constraints.hs\#L247-L298}}
relies on the type-checker to call it at the point where it is needed
and has to be carefully engineered to work with the rest of the
codebase.

\hypertarget{sec:what-is-our-design-bringing-into-the-picture}{%
\section{A blueprint for extensible
elaborators}\label{sec:what-is-our-design-bringing-into-the-picture}}

Our idea for a new design is to shift focus more towards the constraints
themselves:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We give a stable API for raising constraints that can be called by the
  type-checker, essentially creating an ``ask'' to be fulfilled by the
  solvers. This is not dissimilar to the idea of mapping object-language
  unification variables to host-language ones as done by
  \citet{guidiImplementingTypeTheory2017}, view of the ``asks'' as a
  general effect \citep[chap.~4.4]{bauerEqualityCheckingGeneral2020}, or
  communication between actors \citep{allaisTypOSOperatingSystem2022a}.
\item
  To make the language more modular, we make constraints an extensible
  data type in the style of \citet{swierstraDataTypesCarte2008} and give
  an API to define new solvers with the ability to specify what kinds of
  constraints they can solve.
\end{enumerate}

In the examples in this paper, we follow the bidirectional style of
type-checking, but in practice, the design decisions are agnostic of the
underlying system, as long as it adheres to the principle of stating the
requirements on terms in terms of raising a constraint and not by, say,
pattern-matching on a concrete term representation.

From a birds-eye view the architecture looks as depicted in Figure
\ref{architecture-figure}. The type-checking begins by initializing
state and doing the syntax traversal. The traversal raises the
constraints, and for the moment, the constraints are simply stored. As
soon as we finish the traversal of some block (one declaration in our
case), the solver dispatcher is called. It goes over the set of
constraints, and for each active constraint calls different plugins to
make an attempt at solving it. Each plugin, whether user-supplied
(\texttt{Plugin\ A}) or provided by us (\texttt{unification}) consist of
a handler and a solver. The handler determines if the plugin can
potentially solve a constraint, if so the dispatcher runs the
corresponding solver. All components have some read access to the state,
including handlers which might for example verify that there are no
extra constraints on the metavariable. For the write access: syntax
traversal writes new metavariables to the state and elaborated
definitions; the solver dispatcher writes updated meta-information;
solvers write solutions to the metavariables and can raise new
constraints.

\begin{figure*}
  \center

  \includegraphics[width=\textwidth]{architecture-diagram.png}

  \caption{Architecture diagram}
  \label{architecture-figure}
\end{figure*}

For the moment we need to recompile the project in order to include new
plugins, but this is not necessary and a system that dynamically loads
plugins is possible to implement in a way that is similar to GHC
Plugins\footnote{\href{https://mpickering.github.io/plugins.html}{mpickering.github.io/plugins.html}}
or Accelerate \footnote{\href{https://github.com/tmcdonell/accelerate-llvm/blob/master/accelerate-llvm-native/src/Data/Array/Accelerate/LLVM/Native/Link/Runtime.hs\#L40}{github.com/tmcdonell/accelerate-llvm/blob/master/accelerate-llvm-native/src/Data/Array/Accelerate/LLVM/Native/Link/Runtime.hs}}
\citep{mcdonellTypesafeRuntimeCode2015}.

\hypertarget{sec:bidirectional}{%
\section{Dependently typed calculus and bidirectional
typing}\label{sec:bidirectional}}

In this section, we describe the core of the type system we implement.
We take pi-forall \citep{weirichImplementingDependentTypes2022} as a
basis for the system and extend it with metavariables in core syntax and
implicit arguments in the surface syntax. However, for all other
purposes, we leave the core rules intact and therefore, the core
calculus too.

\hypertarget{basic-language-and-rules}{%
\subsection{Basic language and rules}\label{basic-language-and-rules}}

This is a dependently typed calculus that includes Pi, Sigma and indexed
inductive types.

Here is the internal syntax term data type, apart from service
constructor omissions, like a placeholder \texttt{TRUSTME}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data} \DataTypeTok{Term} \OtherTok{=}
  \CommentTok{{-}{-} type of types Type}
    \DataTypeTok{Type}
  \CommentTok{{-}{-} variables x}
  \OperatorTok{|} \DataTypeTok{Var} \DataTypeTok{TName}
  \CommentTok{{-}{-} abstraction \textbackslash{}x. a}
  \OperatorTok{|} \DataTypeTok{Lam}\NormalTok{ (}\DataTypeTok{Bind} \DataTypeTok{TName} \DataTypeTok{Term}\NormalTok{)}
  \CommentTok{{-}{-} application a b}
  \OperatorTok{|} \DataTypeTok{App} \DataTypeTok{Term} \DataTypeTok{Arg}
  \CommentTok{{-}{-} function type (x : A) {-}\textgreater{} B}
  \OperatorTok{|} \DataTypeTok{Pi} \DataTypeTok{Epsilon} \DataTypeTok{Type}\NormalTok{ (}\DataTypeTok{Bind} \DataTypeTok{TName} \DataTypeTok{Type}\NormalTok{)}
  \CommentTok{{-}{-} Sigma{-}type \{ x : A | B \}}
  \OperatorTok{|} \DataTypeTok{Sigma} \DataTypeTok{Term}\NormalTok{ (}\DataTypeTok{Bind} \DataTypeTok{TName} \DataTypeTok{Term}\NormalTok{)}
  \OperatorTok{|} \DataTypeTok{Prod} \DataTypeTok{Term} \DataTypeTok{Term}
  \OperatorTok{|} \DataTypeTok{LetPair} \DataTypeTok{Term}\NormalTok{ (}\DataTypeTok{Bind}\NormalTok{ (}\DataTypeTok{TName}\NormalTok{, }\DataTypeTok{TName}\NormalTok{) }\DataTypeTok{Term}\NormalTok{)}
  \CommentTok{{-}{-} Equality type  a = b}
  \OperatorTok{|} \DataTypeTok{TyEq} \DataTypeTok{Term} \DataTypeTok{Term}
  \OperatorTok{|} \DataTypeTok{Refl}
  \OperatorTok{|} \DataTypeTok{Subst} \DataTypeTok{Term} \DataTypeTok{Term}
  \OperatorTok{|} \DataTypeTok{Contra} \DataTypeTok{Term}
  \CommentTok{{-}{-} inductive datatypes}
  \OperatorTok{|} \DataTypeTok{TCon} \DataTypeTok{TCName}\NormalTok{ [}\DataTypeTok{Arg}\NormalTok{] }\CommentTok{{-}{-} types (fully applied)}
  \OperatorTok{|} \DataTypeTok{DCon} \DataTypeTok{DCName}\NormalTok{ [}\DataTypeTok{Arg}\NormalTok{] }\CommentTok{{-}{-} terms (fully applied)}
  \OperatorTok{|} \DataTypeTok{Case} \DataTypeTok{Term}\NormalTok{ [}\DataTypeTok{Match}\NormalTok{]}
    \CommentTok{{-}{-} metavariables}
  \OperatorTok{|} \DataTypeTok{MetaVar} \DataTypeTok{MetaClosure}
\end{Highlighting}
\end{Shaded}

Equality is not defined as a regular inductive type, but is instead
built-in. The user has access to the type and term constructor, but not
the ability to pattern-matching on it. Instead the language provides a
\texttt{subst} primitive of type
\texttt{(A\ x)\ -\textgreater{}\ (x=y)\ -\textgreater{}\ A\ y} and
\texttt{contra} that takes an equality of two different inductive type
constructors and produces an element of any type.

On top of the above the language includes indexed inductive datatypes
and case-constructs for their elimination. Indexed inductive datatypes
are encoded as parameterised datatypes with an equality argument
constraining the index.

As for metavariables \texttt{MetaVar}: as mentioned in the introduction,
they are placeholders in the syntax tree (AST) that are produced in the
process of elaboration. Metavariables do not appear in the surface
syntax as they are not created by the user. In this paper we implement
metavariables in the contextual style, as described by
\citet{abelHigherOrderDynamicPattern2011}, therefore they have to carry
around a closure of type \texttt{MetaClosure}.

\hypertarget{syntax-traversal}{%
\subsection{Syntax traversal}\label{syntax-traversal}}

We implement the core of the elaborator as a bidirectional syntax
traversal, raising a constraint every time we need to assert something
about the type.

This includes the expected use of unification constraints, like in case
we enter checking mode while the term should be inferred:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{checkType tm ty }\OtherTok{=} \KeywordTok{do}
\NormalTok{  (etm, ty\textquotesingle{}) }\OtherTok{\textless{}{-}}\NormalTok{ inferType tm}
\NormalTok{  constrainEquality ty\textquotesingle{} ty }\DataTypeTok{I.Type}
  \FunctionTok{return}\NormalTok{ etm}
\end{Highlighting}
\end{Shaded}

Any time we want to decompose the type provided in checking mode we pose
a constraint that guarantees the type being in a specific shape:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{checkType (}\DataTypeTok{S.Lam}\NormalTok{ lam) ty }\OtherTok{=} \KeywordTok{do}
\NormalTok{  mtyA }\OtherTok{\textless{}{-}}\NormalTok{ createMetaTerm}
\NormalTok{  mtx }\OtherTok{\textless{}{-}}\NormalTok{ createUnknownVar}
\NormalTok{  mtyB }\OtherTok{\textless{}{-}}\NormalTok{ extendCtx (}\DataTypeTok{I.TypeSig}\NormalTok{ (}\DataTypeTok{I.Sig}\NormalTok{ mtx mtyA))}
\NormalTok{                    (createMetaTerm)}
  \KeywordTok{let}\NormalTok{ mbnd }\OtherTok{=}\NormalTok{ bind mtx mtyB}
  \KeywordTok{let}\NormalTok{ metaPi }\OtherTok{=} \DataTypeTok{I.Pi}\NormalTok{ mtyA mbnd}

\NormalTok{  constrainEquality ty metaPi }\DataTypeTok{I.Type}
  \CommentTok{{-}{-} rest of the traversal can now use mtyA and mbnd}
  \OperatorTok{...}
\end{Highlighting}
\end{Shaded}

At certain points we have to raise a constraint which has an associated
continuation. Like for checking the type of a data constructor -- the
part of the program that comes as an argument to
\texttt{constrainTConAndFreeze} will be suspended (or ``blocked'') until
the meta is solved with something of shape \texttt{TCon\ \_\ \_}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{checkType t}\OperatorTok{@}\NormalTok{(}\DataTypeTok{S.DCon}\NormalTok{ c args) ty }\OtherTok{=} \KeywordTok{do}
\NormalTok{  elabpromise }\OtherTok{\textless{}{-}}\NormalTok{ createMetaTerm}
\NormalTok{  constrainTConAndFreeze ty }\OperatorTok{$} \KeywordTok{do}
\NormalTok{    mty }\OtherTok{\textless{}{-}}\NormalTok{ substMetas ty}
    \KeywordTok{case}\NormalTok{ mty }\KeywordTok{of}
\NormalTok{      (}\DataTypeTok{I.TCon}\NormalTok{ tname params) }\OtherTok{{-}\textgreater{}} \KeywordTok{do}
        \OperatorTok{...}
\NormalTok{      \_ }\OtherTok{{-}\textgreater{}} \FunctionTok{error} \StringTok{"impossible"}
\end{Highlighting}
\end{Shaded}

We add one more case to the elaborator for implicit arguments.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{checkType (}\DataTypeTok{Implicit}\NormalTok{) ty }\OtherTok{=} \KeywordTok{do}
\NormalTok{  m }\OtherTok{\textless{}{-}}\NormalTok{ createMetaTerm}
\NormalTok{  raiseConstraint }\OperatorTok{$} \DataTypeTok{FillInTheMeta}\NormalTok{ m ty}
  \FunctionTok{return}\NormalTok{ m}
\end{Highlighting}
\end{Shaded}

This is used for implicit arguments as will be described in more detail
in Section \ref{sec:case-implicits}. The syntax traversal does not need
to know anything at all about the implicits. The only thing we require
is that the elaboration of the argument is called with the type
information available. This corresponds to how in bidirectional typing
function application is done in the inference mode but the arguments are
processed in checking mode.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inferType (}\DataTypeTok{App}\NormalTok{ t1 t2) }\OtherTok{=} \KeywordTok{do}
\NormalTok{  (et1, }\DataTypeTok{Pi}\NormalTok{ tyA tyB) }\OtherTok{\textless{}{-}}\NormalTok{ inferType t1}
\NormalTok{  et2 }\OtherTok{\textless{}{-}}\NormalTok{ checkType t2 tyA}
  \FunctionTok{return}\NormalTok{ (}\DataTypeTok{App}\NormalTok{ et1 et2, subst tyB et2)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec:constraints_and_unification}{%
\section{Constraints and
unification}\label{sec:constraints_and_unification}}

In Section \ref{sec:bidirectional} we described the syntax traversal
part of the elaborator, which generates the constraints. In this section
we will go over the constraint datatype needed for the base language,
how unification is implemented and how we can extend the unification
procedure.

\hypertarget{base-constraints}{%
\subsection{Base constraints}\label{base-constraints}}

The datatype of constraints is open which means the user can write a
plugin to extend it. However, we offer a few constraints out of the box
to be able to type-check the base language.

For the purposes of the base language it suffices to have the following.

\begin{itemize}
\item
  A constraint that enforces equality of two terms of a given type

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-} two terms given should be equal}
\KeywordTok{data} \DataTypeTok{EqualityConstraint}\NormalTok{ e }\OtherTok{=}
     \DataTypeTok{EqualityConstraint} \DataTypeTok{Syntax.Term} \DataTypeTok{Syntax.Term}
                        \DataTypeTok{Syntax.Type}
                        \DataTypeTok{Sytax.MetaVarId}
\end{Highlighting}
\end{Shaded}
\item
  A constraint that ensures that a metavariable is resolved eventually:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-} this terms has to be filled in}
\KeywordTok{data} \DataTypeTok{FillInTheTerm}\NormalTok{ e }\OtherTok{=}
     \DataTypeTok{FillInTheTerm} \DataTypeTok{Syntax.Term}
\NormalTok{                   (}\DataTypeTok{Maybe} \DataTypeTok{Syntax.Type}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Lastly, a constraint which ensures that a term is a type constructor:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-} the term passed to the constraint}
\CommentTok{{-}{-} should be a type constructor}
\KeywordTok{data} \DataTypeTok{TypeConstructorConstraint}\NormalTok{ e }\OtherTok{=}
     \DataTypeTok{TypeConstructorConstraint} \DataTypeTok{Syntax.Type}
\end{Highlighting}
\end{Shaded}
\end{itemize}

The type-checker raises them supplying the information necessary, but
agnostic of how they will be solved.

\hypertarget{sec:solvers-interface}{%
\subsection{Interface of the solvers}\label{sec:solvers-interface}}

On the solver side we provide a suite\footnote{In the prototype we
  implement a subset of all unification rules, here are they listed:
  identityPlugin, propagateMetasEqPlugin, reduceLeftPlugin,
  reduceRightPlugin, leftMetaPlugin, rightMetaPlugin,
  typeConstructorPlugin, typeConstructorWithMetasPlugin,
  piEqInjectivityPlugin, tyEqInjectivityPlugin, consInjectivityPlugin,
  typeInjectivityPlugin, unificationStartMarker, unificationEndMarker}
of solvers for unification that handle different cases of the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-} solves syntactically equal terms}
\OtherTok{syntacticHandler ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ c)}
                 \OtherTok{=\textgreater{}} \DataTypeTok{HandlerType}\NormalTok{ c}
\OtherTok{syntacticSolver ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ c)}
                \OtherTok{=\textgreater{}} \DataTypeTok{SolverType} \DataTypeTok{Bool}
\OtherTok{syntactic ::} \DataTypeTok{Plugin}
\NormalTok{syntactic  }\OtherTok{=} \DataTypeTok{Plugin}\NormalTok{ \{ solver  }\OtherTok{=}\NormalTok{ syntacticSolver}
\NormalTok{                    , handler }\OtherTok{=}\NormalTok{ syntacticHandler}
                    \OperatorTok{...}
\NormalTok{                    \}}
\end{Highlighting}
\end{Shaded}

We first define the class of constraints that will be handled by the
solver via providing a ``handler'' -- function that decides whether a
given solver has to fire.\footnote{The code shown above and in the rest
  of the paper is close to the actual implementation, but has been
  simplified for presentation purposes.} In this case, this amounts to
checking that the constraint given is indeed an
\texttt{EqualityConstraint} and that the two terms given to it are
syntactically equal. Then we define the solver itself, which in this
case does not have to do anything except \texttt{return\ True} to
indicate that the constraint is solved, as we it only fires once it has
been cleared to do so by the handler and the equality has already been
checked. Finally, we register the solver by declaring it using a plugin
interface.

The reason for this separation between a decision procedure and the
execution of solver is to ensure separation between pontentially slow,
effectful solving and fast read-only decision-making in the handler. We
opt for this division since handlers will be run on many constraints
that do not fit, therefore any write effects would have to be rolled
back, while solvers should be fired only in cases when we can reasonably
hope that the constraint will be solved and the effects will not have to
be rolled back.

Similarly, we can define \texttt{leftMetaSolver} and
\texttt{rightMetaSolver} that only work on problems where only one of
the sides is a metavariable. Here the job of the solver is not as
trivial -- it has to check that the type of the other side indeed
matches the needed one and then register the instantiation of the
metavariable in the state.

Since the constraints they match on overlap we can provide priority
preferences, using the \texttt{pre} and \texttt{suc} fields of the
plugin interface. They are used to indicate whether the currently
defined plugin should run before or after, respectively, which other
plugins. At the time of running the compiler, these preferences are
loaded into a big pre-order relation for all the plugins, which is then
linearised and used to guide the solving procedure.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{rightMetaPlugin ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                \OtherTok{=\textgreater{}} \DataTypeTok{Plugin}\NormalTok{ cs}
\NormalTok{rightMetaPlugin }\OtherTok{=}
  \DataTypeTok{Plugin}\NormalTok{ \{ handler }\OtherTok{=}\NormalTok{ rightMetaHandler}
\NormalTok{         , solver  }\OtherTok{=}\NormalTok{ rightMetaSolver}
\NormalTok{         , symbol  }\OtherTok{=}\NormalTok{ rightMetaSymbol}
\NormalTok{         , pre }\OtherTok{=}\NormalTok{ []}
\NormalTok{         , suc }\OtherTok{=}\NormalTok{ [leftMetaSymbol]}
\NormalTok{         \}}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec:solvers-implementation}{%
\subsection{Implementation of the solvers and unification
details}\label{sec:solvers-implementation}}

We implement a system close to the one described by
\citet{abelHigherOrderDynamicPattern2011}. We modularise the
implementation by mapping every function call in the simplification
procedure to a raised constraint and every simplification rule to a
separate solver. For example, the ``decomposition of functions''
\citep[fig.~2]{abelHigherOrderDynamicPattern2011} rule is translated to
the following implementation.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{piEqInjectivityHandler ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                       \OtherTok{=\textgreater{}} \DataTypeTok{HandlerType}\NormalTok{ cs}
\NormalTok{piEqInjectivityHandler constr }\OtherTok{=} \KeywordTok{do}
  \KeywordTok{let}\NormalTok{ eqcm }\OtherTok{=}\NormalTok{ match }\OperatorTok{@}\DataTypeTok{EqualityConstraint}\NormalTok{ constr}
  \KeywordTok{case}\NormalTok{ eqcm }\KeywordTok{of}
    \DataTypeTok{Just}\NormalTok{ (}\DataTypeTok{EqualityConstraint}\NormalTok{ pi1 pi2 \_ \_) }\OtherTok{{-}\textgreater{}}
      \KeywordTok{case}\NormalTok{ (pi1, pi2) }\KeywordTok{of}
\NormalTok{        (}\DataTypeTok{I.Pi}\NormalTok{ \_ \_ \_, }\DataTypeTok{I.Pi}\NormalTok{ \_ \_ \_) }\OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{True}
\NormalTok{        \_ }\OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}
\NormalTok{    \_ }\OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}
\end{Highlighting}
\end{Shaded}

The handler is simply checking that both sides of the equality are
indeed Pi-types, and in case either of the matches fails, failure will
be reported and the solver will not be fired. The \texttt{match}
function above comes from the open datatype of constraints, checking if
\texttt{constr} can be projected from \texttt{cs} to
\texttt{EqualityConstraint}.

Now let us take a look at the solver.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{piEqInjectivitySolver ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                      \OtherTok{=\textgreater{}} \DataTypeTok{SolverType}\NormalTok{ cs}
\NormalTok{piEqInjectivitySolver constr }\OtherTok{=} \KeywordTok{do}
  \KeywordTok{let}\NormalTok{ (}\DataTypeTok{Just}\NormalTok{ (}\DataTypeTok{EqualityConstraint}\NormalTok{ (}\DataTypeTok{I.Pi}\NormalTok{ a1 b1)}
\NormalTok{                                (}\DataTypeTok{I.Pi}\NormalTok{ a2 b2) \_ m)) }\OtherTok{=}
\NormalTok{        match }\OperatorTok{@}\DataTypeTok{EqualityConstraint}\NormalTok{ constr}
\NormalTok{  ma }\OtherTok{\textless{}{-}}\NormalTok{ constrainEquality a1 a2 }\DataTypeTok{I.Type}
\NormalTok{  (x, tyB1) }\OtherTok{\textless{}{-}}\NormalTok{ Unbound.unbind b1}
\NormalTok{  (y, tyB2\textquotesingle{}) }\OtherTok{\textless{}{-}}\NormalTok{ Unbound.unbind b2}
  \KeywordTok{let}\NormalTok{ tyB2 }\OtherTok{=}\NormalTok{ Unbound.subst y (}\DataTypeTok{I.Var}\NormalTok{ x) tyB2\textquotesingle{}}
\NormalTok{      mat }\OtherTok{=}\NormalTok{ I.identityClosure ma}
\NormalTok{  mb }\OtherTok{\textless{}{-}}\NormalTok{ extendCtx (}\DataTypeTok{I.TypeSig}\NormalTok{ (}\DataTypeTok{I.Sig}\NormalTok{ x e1 mat)) }\OperatorTok{$}
\NormalTok{                  constrainEquality tyB1 tyB2 }\DataTypeTok{I.Type}
  \KeywordTok{let}\NormalTok{ mbt }\OtherTok{=}\NormalTok{ bind x }\OperatorTok{$}\NormalTok{ I.identityClosure mb}
\NormalTok{  solveMeta m (}\DataTypeTok{I.Pi}\NormalTok{ mat mbt)}
  \FunctionTok{return} \DataTypeTok{True}
\end{Highlighting}
\end{Shaded}

Following pi-forall, we use the unbound-generics\footnote{\href{https://hackage.haskell.org/package/unbound-generics-0.4.3}{hackage.haskell.org/package/unbound-generics-0.4.3}}
library to deal with the names and binders. As it will fire after a
handler returns \texttt{True}, we can assume the pattern-matches will
not fail.

First we constrain the equality of the domain of the Pi-type:
\texttt{a1} and \texttt{a2}. The seemingly spurious metavariable
\texttt{ma} returned from this call serves as an anti-unification
\citep{pfenningUnificationAntiunificationCalculus1991} communication
channel. Every time an equality constraint is created we return a
metavariable that stands for the unified term. This metavariable is used
for unification problems that are created in the extended contexts -- in
this case second argument of the Pi-type, but also when solving
equalities concerning two data constructors. We do this to tackle the
``spine problem''
\citep[sec.~1.4]{victorlopezjuanPracticalHeterogeneousUnification2021}
-- as we operate according to the ``well-typed modulo constraints''
principle, essentially providing a placeholder that is guaranteed to
preserve well-typedness in the extended context. \footnote{\(\text{Tog}^{+}\)
  \citep{victorlopezjuanTog2020, victorlopezjuanPracticalHeterogeneousUnification2021}
  focuses on extending unification algorithm for the case where two
  sides of equality might not be of the same type, which is also a
  problem relevant for us. Their main argument against the usage of
  anti-unification in Agda provided there is that it is bug-prone. We
  think that in Agda the problems were stemming from the fact that
  anti-unification were implemented separately from unification, in
  which case it is indeed hard to keep the two in sync. There is no such
  duplication in our case since unification and anti-unification are
  one.} Finally \texttt{ma} has to be applied to a closure which will
keep track of the delayed substitution.

Then we can constrain the co-domains of Pi-types in an extended context.
In case one of solvers the constraints created in the extended context
might need to know the exact shape of \texttt{ma}, we can block on the
metavariable later, freezing the rest of the problem until it is
instantiated.

As for the actual unification steps, we implement them in a similar
fashion to simplification procedure. Take a look at the following
example, when only the left hand side of the constaint is an unsolved
meta:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{leftMetaSolver ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
               \OtherTok{=\textgreater{}} \DataTypeTok{SolverType}\NormalTok{ cs}
\NormalTok{leftMetaSolver constr }\OtherTok{=} \KeywordTok{do}
  \KeywordTok{let}\NormalTok{ (}\DataTypeTok{Just}\NormalTok{ (}\DataTypeTok{EqualityConstraint}\NormalTok{ t1 t2 \_ m)) }\OtherTok{=}
\NormalTok{        match }\OperatorTok{@}\DataTypeTok{EqualityConstraint}\NormalTok{ constr}
\NormalTok{      (}\DataTypeTok{MetaVar}\NormalTok{ (}\DataTypeTok{MetaVarClosure}\NormalTok{ m1 c1)) }\OtherTok{=}\NormalTok{ t1}
\NormalTok{  mt2 }\OtherTok{\textless{}{-}}\NormalTok{ occursCheck m1 t2}
  \KeywordTok{case}\NormalTok{ mt2 }\KeywordTok{of}
    \CommentTok{{-}{-} indicates a failure in occurs{-}check}
    \DataTypeTok{Left}\NormalTok{ e }\OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}
    \CommentTok{{-}{-} indicates a passed occurs{-}check}
    \DataTypeTok{Right}\NormalTok{ t2 }\OtherTok{{-}\textgreater{}}
      \KeywordTok{case}\NormalTok{ (invertClosureOn c1 (freeVarList t2)) }\KeywordTok{of}
        \DataTypeTok{Just}\NormalTok{ s }\OtherTok{{-}\textgreater{}} \KeywordTok{do}
          \KeywordTok{let}\NormalTok{ st2 }\OtherTok{=}\NormalTok{ Unbound.substs s t2}
          \CommentTok{{-}{-} apply the subsitution}
\NormalTok{          solveMeta m1 st2}
          \CommentTok{{-}{-} instantiate the anti{-}unification variable}
\NormalTok{          solveMeta m st2}
          \FunctionTok{return} \DataTypeTok{True}
        \DataTypeTok{Nothing} \OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}
\end{Highlighting}
\end{Shaded}

Once the occurs-check returns and if it was successful we apply the
inverted closure to the right-hand side of the equality.

By splitting up the rules into individual, simple solvers we can
compartmentalise the complexity of the unifier, making sure that each
rule is as decoupled from the others as possible. This does not
deteriorate the properties of the system, but does not help to guarantee
them either. We talk more about the challenge of proving correctness in
Section \ref{sec:limitations}.

\hypertarget{extending-unification}{%
\subsection{Extending unification}\label{extending-unification}}

Now that unification is implemented let us create a simple plugin that
makes certain symbols declared by the user injective
\citep{agdausersInjectiveUnificationPragma2023}. The actual
implementation is relatively simple and is not dissimilar to the
Pi-injectivity solver we showed above.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{userInjectivitySolver ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                      \OtherTok{=\textgreater{}} \DataTypeTok{SolverType}\NormalTok{ cs}
\NormalTok{userInjectivitySolver constr }\OtherTok{=} \KeywordTok{do}
  \KeywordTok{let}\NormalTok{ (}\DataTypeTok{Just} \DataTypeTok{EqualityConstraint}
\NormalTok{              (}\DataTypeTok{I.App}\NormalTok{ (}\DataTypeTok{I.Var}\NormalTok{ f) a)}
\NormalTok{              (}\DataTypeTok{I.App}\NormalTok{ (}\DataTypeTok{I.Var}\NormalTok{ g) b) \_ m)) }\OtherTok{=}
\NormalTok{        match }\OperatorTok{@}\DataTypeTok{EqualityConstraint}\NormalTok{ constr}
  \KeywordTok{if}\NormalTok{ f }\OperatorTok{==}\NormalTok{ g}
  \KeywordTok{then} \KeywordTok{do}
\NormalTok{    ifM (queryInjectiveDeclarations f)}
\NormalTok{        (}\KeywordTok{do}
\NormalTok{           ms }\OtherTok{\textless{}{-}}\NormalTok{ constrainEquality a b }\DataTypeTok{I.Type}
\NormalTok{           solveMeta m ms}
           \FunctionTok{return} \DataTypeTok{True}\NormalTok{)}
\NormalTok{        (}\FunctionTok{return} \DataTypeTok{False}\NormalTok{)}
  \KeywordTok{else} \FunctionTok{return} \DataTypeTok{False}
\end{Highlighting}
\end{Shaded}

where \texttt{queryInjectiveDeclarations} simply scans the available
declarations for a marker that \texttt{f} has been declared injective.

The only big thing left is to make sure that this solver fires at the
right time. This can only conflict with the ``decomposition of
neutrals'' rule, so we indicate to the solver dispatcher that our plugin
should run before the it:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{userInjectivityPlugin ::}\NormalTok{ (}\DataTypeTok{EqualityConstraint} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                      \OtherTok{=\textgreater{}} \DataTypeTok{Plugin}\NormalTok{ cs}
\NormalTok{userInjectivityPlugin }\OtherTok{=}
  \DataTypeTok{Plugin}\NormalTok{ \{ }\OperatorTok{...}
\NormalTok{         , solver }\OtherTok{=}\NormalTok{ userInjectivitySolver}
\NormalTok{         , symbol }\OtherTok{=} \DataTypeTok{PluginId} \OperatorTok{$} \StringTok{"userInjectivity"}
\NormalTok{         , pre }\OtherTok{=}\NormalTok{ [ unifyNeutralsDecomposition}
\NormalTok{                 , unificationEndMarkerSymbol]}
\NormalTok{         , suc }\OtherTok{=}\NormalTok{ [unificationStartMarkerSymbol]}
\NormalTok{         \}}
\end{Highlighting}
\end{Shaded}

This modification does not alter the core of the language.

\hypertarget{sec:casestudies}{%
\section{Case studies}\label{sec:casestudies}}

Once we implement basic elaboration and unification we can extend the
language. This is where we make use of the fact that the constraints
datatype is open.

We saw before in Section \ref{sec:implicit-arguments} that conventional
designs require separate handling of different kinds of implicit
variables. To simplify the design we would like to uniformly dispatch a
search for the solution, which would be handled by the a fitting solver.
We can achieve this by communicating the kind of the meta through its
type in the second argument of \texttt{FillInTheMeta\ m\ ty}. The
solvers then match on the shape of the type of the metavariable and
handle it in a case-specific manner: instance-search for type classes,
tactic execution for a tactic argument, waiting for regular unification
to solve the metavariable for regular implicit arguments.

In this section we will describe the implementation details of regular
implicit arguments (Section \ref{sec:case-implicits}), the
implementation of type classes added on top of the implicit arguments
(Section \ref{sec:case-typeclasses}). And finally (Section
\ref{sec:coercion-tactics}) we sketch the implementation of coercive
subtyping and tactic arguments.

\hypertarget{sec:case-implicits}{%
\subsection{Implicit arguments}\label{sec:case-implicits}}

As we mentioned, the rule for \texttt{Implicit} had to be added to the
syntax traversal part of the elaborator. In fact, we require not one but
two modifications that lie outside of the solvers-constraints part of
the system. The first one is, indeed, the addition of a separate case in
the syntax traversal, however contained. The second one lies in the
purely syntactical part of the compiler. We need the pre-processor to
insert the placeholder terms in the surface syntax. Particularly, we
need to desugar declarations of functions in the following way:

For any declaration of a function \texttt{f} with some implicit argument
\texttt{a}:

\begin{verbatim}
def f : {A : Type} -> {a : A} -> B a -> C
\end{verbatim}

Which should desugar to the following:

\begin{verbatim}
def f : (A : Implicit Type)
     -> (a : Implicit (deImp A))
     -> (b : B (deImp a)) -> C
\end{verbatim}

Then, for each function call \texttt{f\ b1} we need to insert the
corresponding number of implicit arguments, transforming it to
\texttt{f\ \_\ \_\ b1}.

For simple implicits this suffices -- as soon as we have the
placeholders in the surface syntax we create the constraints in the
\texttt{Implicit} case of the syntax traversal. The only solver that is
needed in this case is a trivial one that checks that the metavariable
has been instantiated indeed. This is because a regular implicit should
be instantiated by the unifier at some point later. This constraint
simply serves as a guarantee that all implicits have been instantiated.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{fillInImplicitSymbol ::} \DataTypeTok{PluginId}

\OtherTok{fillInImplicitHandler ::}\NormalTok{ (}\DataTypeTok{FillInImplicit} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                      \OtherTok{=\textgreater{}} \DataTypeTok{HandlerType}\NormalTok{ cs}
\NormalTok{fillInImplicitHandler constr }\OtherTok{=} \KeywordTok{do}
  \KeywordTok{let}\NormalTok{ ficm }\OtherTok{=}\NormalTok{ match }\OperatorTok{@}\DataTypeTok{FillInImplicit}\NormalTok{ constr}
  \KeywordTok{case}\NormalTok{ ficm }\KeywordTok{of}
    \DataTypeTok{Just}\NormalTok{ (}\DataTypeTok{FillInImplicit}\NormalTok{ term ty) }\OtherTok{{-}\textgreater{}} \KeywordTok{do}
      \KeywordTok{case}\NormalTok{ term }\KeywordTok{of}
        \DataTypeTok{MetaVar}\NormalTok{ (}\DataTypeTok{MetaVarClosure}\NormalTok{ mid \_) }\OtherTok{{-}\textgreater{}}
\NormalTok{          isMetaSolved mid}
\NormalTok{        \_ }\OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}
    \DataTypeTok{Nothing} \OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}

\OtherTok{fillInImplicitPlugin ::}\NormalTok{ (}\DataTypeTok{FillInImplicit} \OperatorTok{:\textless{}:}\NormalTok{ cs)}
                     \OtherTok{=\textgreater{}} \DataTypeTok{Plugin}\NormalTok{ cs}
\NormalTok{fillInImplicitPlugin }\OtherTok{=} \DataTypeTok{Plugin}\NormalTok{ \{}
\NormalTok{  solver }\OtherTok{=}\NormalTok{ fillInImplicitSolver,}
\NormalTok{  handler }\OtherTok{=}\NormalTok{ fillInImplicitHandler,}
\NormalTok{  symbol }\OtherTok{=}\NormalTok{ fillInImplicitSymbol,}
\NormalTok{  suc }\OtherTok{=}\NormalTok{ [],}
\NormalTok{  pre }\OtherTok{=}\NormalTok{ []}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

Additionally, there is a design choice to be made the implementation of
\texttt{Implicit\ A} and \texttt{deImp}. One option is to turn them into
a constructor and a projection of a record type, the other is to make
them computationally equivalent to \texttt{id}. In the former case we
need to manually unwrap them both in the pre-processor and during the
constraint-solving, but since the head symbol is distinct we can
guarantee that other solvers will not match on it, unless explicitly
instructed to. In the latter case, one has to be cautious of the order
in which the solvers are activated, particularly in the case of
different search procedures, should they be implemented. However, in the
example above it does not make a difference.

\hypertarget{sec:case-typeclasses}{%
\subsection{Type classes}\label{sec:case-typeclasses}}

Next, let us implement a plugin for type classes. Same as for the
implicit arguments in general, we rely here on a pre-processor to
transform user-friendly syntax to simple declarations. We do not focus
on this part, since such a pre-processor does a simple local
transformation.

Let us now go through an example of the elaboration process for a simple
term. For typeclasses we need a few declarations, listed below in
Agda-like syntax:

\begin{verbatim}
plus  :  {A : Type} -> {{PlusOperation A}}
     -> (a : A) -> (b : A) -> A

-- a semigroup on booleans, addition is OR
instance BoolPlus : PlusOperation Bool where
  plus = orb
\end{verbatim}

And the exemplary term itself is:

\begin{verbatim}
m = plus True False
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  First, the pre-processor eliminates the implicits and type class
  arguments. We end with the following declarations:

\begin{verbatim}
plus : (impA : Implicit Type)
    -> Instance PlusOperation (deImp impA)
    -> (a : deImp impA) -> (b :  deImp impA)
    -> deImp impA

instanceBoolPlus : InstanceT PlusOperation Bool
instanceBoolPlus =
  InstanceC (TypeClassC (Plus orb))

m = plus _ _ True False
\end{verbatim}

  Declaration \texttt{instance\ BoolPlus} turning into a constructor of
  \texttt{InstanceC} is precisely the part we need the pre-processor to
  do.
\item
  We go into the elaboration of \texttt{m} now. The elaborator applies
  \texttt{inferType\ (App\ t1\ t2)} rule four times and
  \texttt{checkType\ (Implicit)\ ty} twice on the two placeholders. The
  output of the elaborator is

\begin{verbatim}
m = plus ?_1 ?_2 True False
\end{verbatim}

  And the state of the elaborator contains four more constraints:

\begin{verbatim}
C1: FillInTheTerm ?_1 (Implicit Type)
C2: FillInTheTerm ?_2 (InstanceT PlusOperation
                                 (deImp ?_1))
C3: EqualityConstraint (deImp ?_1) Bool Type
C4: EqualityConstraint (deImp ?_1) Bool Type
\end{verbatim}

  The first two correspond to implicit arguments. The latter two are
  unification problems rendered into constraints.
\item
  Now we step into the constraint-solving world. First, the unifier
  solves the latter two constraints, instantiating \texttt{?\_1} to
  \texttt{Implicit\ Bool}. \texttt{C1} is then discarded as solved since
  \texttt{?\_1} is already instantiated to \texttt{Implicit\ Bool}.
  Next, the typeclass resolution launches a search for the instance of
  type \texttt{Instance\ PlusOperation\ Bool}.
\item
  This is where the type class plugin can take over. It transforms
  \texttt{C2:\ FillInTheTerm\ ?\_2\ (InstanceT\ PlusOperation\ Bool)} to
  \texttt{C5:\ InstanceSearch\ PlusOperation\ Bool\ ?\_2}. \texttt{C5}
  then gets matched by the search plugin for concrete instances, simply
  weeding through available declarations, looking for something of the
  shape \texttt{InstanceT\ PlusOperation\ Bool}. Such a declaration
  exists indeed and we can instantiate \texttt{?\_2} to
  \texttt{instanceBoolPlus}.
\end{enumerate}

Now let us take a look at the plugin for the constraint system. It is
contained in a single file
\href{https://github.com/liesnikov/extensible-elaborator/blob/elaborator-experiments/exel/src/Plugins/Typeclasses.hs}{\texttt{./exel/src/Plugins/Typeclasses.hs}}.
In it we define a new constraint type \texttt{InstanceSearch}, a solver
that transforms constraints of the shape
\texttt{FillInTheType\ ?\ (InstanceT\ \_)} to the new constraint, and
finally, a solver for instance search problems. The reason to transform
the original constraint to the new type is to ensure that no other
solver will make an attempt at this problem, therefore modulating
interactions with other plugins.

Finally, the implementation of search for concrete instances is quite
simple:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{instanceConcreteSolver constr }\OtherTok{=} \KeywordTok{do}
  \KeywordTok{let}\NormalTok{ (}\DataTypeTok{Just}\NormalTok{ (}\DataTypeTok{InstanceSearch}\NormalTok{ tcn ty m)) }\OtherTok{=}
\NormalTok{        match }\OperatorTok{@}\DataTypeTok{InstanceSearch}\NormalTok{ constr}
\NormalTok{  alldecls }\OtherTok{\textless{}{-}}\NormalTok{ collectInstancesOf tcn }\OperatorTok{\textless{}$\textgreater{}}\NormalTok{ SA.getDecls}
\NormalTok{  sty }\OtherTok{\textless{}{-}}\NormalTok{ SA.substAllMetas ty}
  \KeywordTok{case}\NormalTok{ Map.lookup sty alldecls }\KeywordTok{of}
    \DataTypeTok{Just}\NormalTok{ i }\OtherTok{{-}\textgreater{}} \KeywordTok{do}
\NormalTok{      SA.solveMeta m (}\DataTypeTok{I.Var}\NormalTok{ i)}
      \FunctionTok{return} \DataTypeTok{True}
    \DataTypeTok{Nothing} \OtherTok{{-}\textgreater{}} \FunctionTok{return} \DataTypeTok{False}
\end{Highlighting}
\end{Shaded}

We conjecture that implementation of Canonical Structures
\citep{mahboubiCanonicalStructuresWorking2013} would be relatively
simple in such a system due to openness of both unification procedure
and instance search.

\hypertarget{sec:coercion-tactics}{%
\subsection{Tactic arguments and coercions}\label{sec:coercion-tactics}}

In a similar fashion to the transformation of \texttt{FillInImplicit}
constraints to \texttt{InstanceSearch}, we could implement tactic
arguments and coercive subtyping.

The former would be quite similar to what we saw in the previous section
\ref{sec:case-typeclasses}, except we would have to resolve
\texttt{FillInImplicit} to \texttt{RunTactic} constraint or fill in
directly. The question of actually running the tactics is independent of
constraints, since it is essentially another type-checking action that
we accomodate already for blockers.

Coercive subtyping is of a slightly different nature. First, it would
rely on a pre-processor the heaviest out of all of the examples
described above, since naively one would insert a (potentially identity)
coercion in each argument of the application and potentially around
heads of applications and types of abstractions
\citep{tassiBiDirectionalRefinementAlgorithm2012}. This incurs not only
syntactic noise, but also potential performance penalty, which we
describe further in Section \ref{sec:limitations}. The second challenge
comes from the fact that unlike the above this feature would be
anti-modular in a sense that we need to access several constraints at
once, at least read-only. Consider the following example:

\begin{verbatim}
f : (A : Type) -> A -> A
arg : ArgTy

t : ExpTy
t = f _ arg
\end{verbatim}

This declaration would desugar to something like

\begin{verbatim}
t = coerce _ (f _ (coerce _ arg))
\end{verbatim}

which gives raise to two constraints, where \texttt{Coercion\ A\ B}
stands for the evidence of \texttt{A} being a subtype of \texttt{B}:

\begin{verbatim}
C1 : Coercion ArgTy ?1
C3 : Coercion ?1 ExpTy
\end{verbatim}

Clearly, we can not solve the first constraint in isolation and need to
consider all coercion constraints related to a particular type at the
same time. While we can accommodate such solvers, due to solvers having
write-access to the state, it becomes much harder to modulate
interactions between different plugins.

\hypertarget{sec:limitations}{%
\section{Limitations}\label{sec:limitations}}

While our design offers a lot of flexibility, it does not solve every
problem. In this section we describe a few examples of extensions that
do not quite fit in this framework and general limitations.

\hypertarget{handling-of-meta-variables-outside-of-definition-sites}{%
\subsection{Handling of meta-variables outside of definition
sites}\label{handling-of-meta-variables-outside-of-definition-sites}}

After we elaborate a definition there can still be unsolved
metavariables in it. This presents us with a design choice. The first
option is to freeze the metavariables and instantiate them per-use site,
essentially allowing for an expression to have multiple types. The
second option is to leave them up to be solved later, which might make
elaboration less predictable since now the use sites can influence
whether a particular definition type-checks. The third option is to
report them as an error and exit immediately.

In particular, the second option allows us to incorporate more involved
inference algorithms into the system. For example, if we were to
implement an erasure inference algorithm as described by
\citet{tejiscakDependentlyTypedCalculus2020}, we would have to create
metavariable annotations (described as ``evars'' in the paper) that can
be instantiated beyond the definition site. The downside is that the
defintions can change depending on their use and type-checking has to be
done effectively for the whole program and not per-definition.

\hypertarget{the-language-is-only-as-extensible-as-the-syntax-traversal-is}{%
\subsection{The language is only as extensible, as the syntax traversal
is}\label{the-language-is-only-as-extensible-as-the-syntax-traversal-is}}

Extensibility via constraints allows for a flexible user-specified
control flow as soon as we step into the constraints world. But control
flow of the syntax traversal for the basic language elaborator is fixed
by the basic language. For example, consider the following simplified
lambda-function type-checking function\footnote{\href{https://github.com/agda/agda/blob/v2.6.4/src/full/Agda/TypeChecking/Rules/Term.hs\#L430-L518}{./src/full/Agda/TypeChecking/Rules/Term.hs\#L430-L518}}
from Agda:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{checkLambda\textquotesingle{} cmp b xps typ body target }\OtherTok{=} \KeywordTok{do}
  \DataTypeTok{TelV}\NormalTok{ tel btyp }\OtherTok{\textless{}{-}}\NormalTok{ telViewUpTo numbinds target}
  \KeywordTok{if}\NormalTok{ size tel }\OperatorTok{\textless{}}\NormalTok{ numbinds }\OperatorTok{||}\NormalTok{ numbinds }\OperatorTok{/=} \DecValTok{1}
    \KeywordTok{then}\NormalTok{ dontUseTargetType}
    \KeywordTok{else}\NormalTok{ useTargetType tel btyp}
\end{Highlighting}
\end{Shaded}

Here Agda steps away from the bidirectional discipline and infers a
(lambda) function if the target type is not fully known. If in our
design the developer chooses to go only with a pure bidirectional style
of type-checking inferred lambda functions would be impossible to
emulate.

The solution in this case is effectively to replicate what Agda is doing
by implementing each type-checking rule in inference mode, essentially
factoring out \texttt{dontUseTargetType} in Agda's code snippet above.

Similarly, implementing something like commutativity and associativity
unifier plugins can require modifications in the core, since we two
terms that are equal during elaboration have to equal during core
type-checking too.

\hypertarget{lack-of-backtracking}{%
\subsection{Lack of backtracking}\label{lack-of-backtracking}}

We do not implement any backtracking in the solver dispatcher as it is
now. This means that every step taken is committing, which can be a
limitation in cases where one would like to have backtracking -- for
example, Agda's instance arguments search (with
\texttt{-\/-overlapping-instances} flag)
\citep[chap.~3.18]{theagdateamAgdaUserManual2023a}, as well as type
classes in Lean \citep{selsamTabledTypeclassResolution2020}.

Backtracking in principle could be achieved by tracking changes to the
state of the elaborator and the production graph for constraints, but
such a system would be rather awkward. Alternatively, such a solver can
be implemented within one solver, removing the extension points, but
allowing arbitrary control-flow within the algorithm.

\hypertarget{reliance-on-a-pre-processor}{%
\subsection{Reliance on a
pre-processor}\label{reliance-on-a-pre-processor}}

This work crucially relies on a pre-processor of some kind, be it macro
expansion or some other way to extend the parser with custom desugaring
rules. In particular, in order to implement n-ary implicit arguments
correctly and easily we need the pre-processor to expand them to the
right arity, similar to
Matita\citep[chap.~5]{tassiBiDirectionalRefinementAlgorithm2012} and
others
\citep{serranoQuickLookImpredicativity2020, kovacsElaborationFirstclassImplicit2020}.

\hypertarget{eager-reduction-and-performance}{%
\subsection{Eager reduction and
performance}\label{eager-reduction-and-performance}}

As in the pre-processing step we would have to insert a fair number of
wrappers and un-wrappers for all implicits and even more for coercions,
a performance regression would be expected due a lot of spurious
computation steps.

We expect this to be a major concern for coercive subtyping. As one way
to mitigate we suggest a discipline with constraint solvers latching
onto non-reduced types and terms in constraints. In that case, we can
get around with a trick borrowed from Coq, where the wrappers and
unwrappers are identity functions and \texttt{coerce\ f\ t} computes to
\texttt{f\ t}. This also means that constraints can/have to match on
unreduced types in the e.g.~\texttt{FillInTheTherm}. In fact, we already
do this to an extent for a different reason -- since the calculus allows
only fully applied type constructors, we have to wrap each type class
constructor in a lambda-abstraction for it to appear as an argument to
\texttt{TypeClassT\ typeClassName\ argType}. Or, concretely, we have to
define and use
\texttt{PlusOperation\textquotesingle{}\ =\ \textbackslash{}A\ .\ PlusOperation\ A}
in place of \texttt{PlusOperation} in the elaboration example in Section
\ref{sec:case-typeclasses}.

\hypertarget{proving-correctness}{%
\subsection{Proving correctness}\label{proving-correctness}}

As soon as we allow the users to implement their own solvers, there is
little we can say about correctness of the system as a whole without
imposing proof obligations on the plugin writers. However, if one is
willing to do that we can conjecture solution and type preservation --
Theorems 2 and 3, respectively \citep{abelHigherOrderDynamicPattern2011}
-- assuming that each solver on its own satisfies these properties. For
termination it is similar -- we can conjecture that if every solver in
the system ``reduces the weight'' of the unification problem, in terms
of Theorem 1 by \citet{abelHigherOrderDynamicPattern2011}, we can hope
for termination guarantees. This is simply a consequence of the fact
that we do not invent a new unification algorithm, but rather provide
means of easier implementation for it.

\hypertarget{sec:related_work}{%
\section{Related work}\label{sec:related_work}}

We are certainly not the first ones to try to tackle the extensibility
of a language implementation. Dependently typed language implementations
usually consist of at least four parts: parser, elaborator, core
type-checker, and proper back-end. The back-end part is currently
irrelevant to our interests, since for a language to be specified
usually means for specification of the core, anything that happens after
the core does not extend the language, but rather tries to preserve its
semantics in some form. Therefore we're left with three parts: parser,
elaborator, and core type-checker.

We see parser or syntax extensibility as a necessary part of an
extensible language. This problem has been studied extensively in the
past and has a multitude of existing solutions. Macros are one of them
and are utilised heavily in various forms in almost all established
languages
\citep{thecoqdevelopmentteamCoqProofAssistant2022, theagdateamAgdaUserManual2023a, ullrichNotationsHygienicMacro2020}
and can be powerful enough to build a whole language around
\citep{changDependentTypeSystems2019}.

Core extensibility, on the other hand, appears to be a problem with too
many degrees of freedom. Andromeda
\citep{bauerDesignImplementationAndromeda2018, bauerEqualityCheckingGeneral2020}
made an attempt at extensible definitional equality but is quite far
from a usable dependently typed language. Agda's philosophy allows
developers to experiment with the core but also results in a larger
amount of unexpected behaviours. In general, modifications of the core
rules will result in fundamental changes in the type theory, which can
break plenty of important properties like soundness or subject
reduction.

This leaves us with the question about the extensibility of an
elaborator. We will make a division here between syntax traversals,
constraint solving and all other features. The syntax traversal part of
the elaborator is relatively stable and commonly implemented following a
bidirectional discipline to some extent
\citep{norellPracticalProgrammingLanguage2007, tassiBiDirectionalRefinementAlgorithm2012, ferreiraBidirectionalElaborationDependently2014},
so there seems little reason to make it extensible.

GHC has a plugin system that allows users to dynamically add custom
constraint solvers, but the type of constraints itself is not
extensible\footnote{\href{https://gitlab.haskell.org/ghc/ghc/-/wikis/plugins/type-checker/notes}{gitlab.haskell.org/ghc/ghc/-/wikis/plugins/type-checker/notes}}
\citep{peytonjonesTypeInferenceConstraint2019, vytiniotisOutsideInModularType2011, peytonjonesPracticalTypeInference2007}.

Coq \citep{thecoqdevelopmentteamCoqProofAssistant2022}, being one of the
most popular proof assistants, invested a lot effort into user-facing
features: work on tactics like a new tactic engine
\citep{spiwackVerifiedComputingHomological2011} and tactic languages
(Ltac2 \citep{pedrotLtac2TacticalWarfare2019}, SSReflect
\citep{gonthierSmallScaleReflection2008}, etc.), the introduction of a
virtual machine for performance
\citep{gregoireCompiledImplementationStrong2002} and others. However,
the implementation is quite hard to extend. One either has to modify the
source code, which is mostly limited to the core development team, as
seen from the
\href{https://github.com/coq/coq/graphs/contributors}{graph}. Or one has
to use Coq plugins system, which is rather challenging, and in the end
the complexity of it gave rise to TemplateCoq
\citep{malechaExtensibleProofEngineering2014}.

Agda has historically experimented a lot with different extensions for
both type system and the elaborator, even though the design doesn't
accomodate these changes naturally. Instead, each of these extensions is
spread throughout many different parts of the code-base\footnote{some
  recent examples:
  \href{https://github.com/agda/agda/pull/6385}{github.com/agda/agda/pull/6385},
  \href{https://github.com/agda/agda/pull/6354}{github.com/agda/agda/pull/6354}.}.

Lean introduced elaborator extensions
\citep{leonardodemouraLeanMetaprogramming2021, ullrichNotationsHygienicMacro2020}.
They allow the user to overload the commands, but if one defines a
particular elaborator it becomes hard to interleave with others. In a
way, this is an imperative view on extensibility.

Idris
\citep{bradyIdrisGeneralpurposeDependently2013, christiansenElaboratorReflectionExtending2016}
appeared as a programming language first and proof-assistant second and
does not provide either a plugin or hook system at all, except for
reflection. Idris also focuses on tactics as the main mechanism for
elaboration.

Turstile+ by \citet{changDependentTypeSystems2019} uses macros to
elaborate surface syntax to a smaller core. Macros allow them to
modularly implement individual features, however, exactly as for Lean,
combining different features which happens a lot in practise requires
the user to re-define all macros from scratch.

TypOS \citep{allaisTypOSOperatingSystem2022a, guillaumeallaisTypOS2022}
is perhaps the closest to our work, but there are two important
differences. First, they are building a domain-specific language for
building type-checkers, while our design is language-agnostic, as long
as it can model extensible datatypes in some capacity. Second, their
approach settles features of the language as they are decided by the
main developer and doesn't concern future changes and evolution.
Finally, we try to stay close to the designs of existing dependently
typed languages, while TypOS doesn't showcase support for them.

\hypertarget{future-work}{%
\section{Future work}\label{future-work}}

We see three main prospects for future work:

\begin{itemize}
\tightlist
\item
  \textbf{Exploration of different kinds of metavariables.} Currently we
  implement metavariables only for terms, while for some applications
  such as erasure \citep{tejiscakDependentlyTypedCalculus2020} or
  irrelevance inference, it would be beneficial to have metavariables
  representing erasure and relevance annotations. Alternatively, we can
  introduce metavariables for names and implement data constructor
  disambiguation in a simpler manner, since as of now we simply block on
  expected type to disambiguate.
\item
  \textbf{Rendering more elements of the elaborator as constraints} --
  currently components such as occurs-checker and reduction are simple
  functions. Including them into the constraint machinery would make the
  implementation more uniform and allow users to extend them.
\item
  \textbf{Potential optimisations}. Currently our system has a lot of
  room for potential optimisations. The first step would be to allow
  handlers to pass some information to their respective solvers, which
  require introduction an existential type in the plugin. Additionally
  some expandable per-plugin store for the solvers would be handy, for
  example to not recompute type class instances on every invocation.
  Somewhat more challenging, one can imagine a caching system for
  constraints, allowing to avoid solving the same constraint twice. This
  might be beneficial for reduction, since as of now we do a lot of
  redundant computations. However, the memory usage might be
  prohibitive. Finally, we would also like to explore possibilities for
  concurrent solving, similar to the future plans of
  \citet{allaisTypOSOperatingSystem2022a} with LVars for metavariables
  \citep{kuperLatticebasedDataStructures2015}.
\end{itemize}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bib.bib}

\end{document}
